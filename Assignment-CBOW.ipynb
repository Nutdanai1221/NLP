{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec (CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Load data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### corpus data from wiki "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"An orange is fruit that has sour taste and rough skin\", \n",
    "          \"A mango is fruit that has sweet taste and smooth skin\", \n",
    "          \"A grape is fruit that has sweet taste and smooth skin\", \n",
    "          \"The cat is animal that live in home and like to eat rat\", \n",
    "          \"Fish is animal that live in the sea or river, it can breath under the water\", \n",
    "          \"Rat is animal that live in everywhere, it like to eat trash\"]\n",
    "          "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized = [sent.split(\" \") for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "flattened_corpus = flatten(corpus_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mango',\n",
       " 'Fish',\n",
       " 'live',\n",
       " 'eat',\n",
       " 'under',\n",
       " 'the',\n",
       " 'to',\n",
       " 'animal',\n",
       " 'everywhere,',\n",
       " 'taste',\n",
       " 'has',\n",
       " 'river,',\n",
       " 'The',\n",
       " 'sea',\n",
       " 'it',\n",
       " 'An',\n",
       " 'trash',\n",
       " 'cat',\n",
       " 'orange',\n",
       " 'grape',\n",
       " 'skin',\n",
       " 'like',\n",
       " 'or',\n",
       " 'breath',\n",
       " 'sweet',\n",
       " 'in',\n",
       " 'that',\n",
       " 'can',\n",
       " 'water',\n",
       " 'sour',\n",
       " 'is',\n",
       " 'rat',\n",
       " 'A',\n",
       " 'and',\n",
       " 'rough',\n",
       " 'fruit',\n",
       " 'smooth',\n",
       " 'Rat',\n",
       " 'home']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs = list(set(flattened_corpus))\n",
    "vocabs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Numerization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_idx = {v: idx for idx, v in enumerate(vocabs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_2_idx[\"rat\"]\n",
    "len(word_2_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs.append('<UNK>')\n",
    "word_2_idx['<UNK>'] = 39 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mango': 0,\n",
       " 'Fish': 1,\n",
       " 'live': 2,\n",
       " 'eat': 3,\n",
       " 'under': 4,\n",
       " 'the': 5,\n",
       " 'to': 6,\n",
       " 'animal': 7,\n",
       " 'everywhere,': 8,\n",
       " 'taste': 9,\n",
       " 'has': 10,\n",
       " 'river,': 11,\n",
       " 'The': 12,\n",
       " 'sea': 13,\n",
       " 'it': 14,\n",
       " 'An': 15,\n",
       " 'trash': 16,\n",
       " 'cat': 17,\n",
       " 'orange': 18,\n",
       " 'grape': 19,\n",
       " 'skin': 20,\n",
       " 'like': 21,\n",
       " 'or': 22,\n",
       " 'breath': 23,\n",
       " 'sweet': 24,\n",
       " 'in': 25,\n",
       " 'that': 26,\n",
       " 'can': 27,\n",
       " 'water': 28,\n",
       " 'sour': 29,\n",
       " 'is': 30,\n",
       " 'rat': 31,\n",
       " 'A': 32,\n",
       " 'and': 33,\n",
       " 'rough': 34,\n",
       " 'fruit': 35,\n",
       " 'smooth': 36,\n",
       " 'Rat': 37,\n",
       " 'home': 38,\n",
       " '<UNK>': 39}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_2_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'mango',\n",
       " 1: 'Fish',\n",
       " 2: 'live',\n",
       " 3: 'eat',\n",
       " 4: 'under',\n",
       " 5: 'the',\n",
       " 6: 'to',\n",
       " 7: 'animal',\n",
       " 8: 'everywhere,',\n",
       " 9: 'taste',\n",
       " 10: 'has',\n",
       " 11: 'river,',\n",
       " 12: 'The',\n",
       " 13: 'sea',\n",
       " 14: 'it',\n",
       " 15: 'An',\n",
       " 16: 'trash',\n",
       " 17: 'cat',\n",
       " 18: 'orange',\n",
       " 19: 'grape',\n",
       " 20: 'skin',\n",
       " 21: 'like',\n",
       " 22: 'or',\n",
       " 23: 'breath',\n",
       " 24: 'sweet',\n",
       " 25: 'in',\n",
       " 26: 'that',\n",
       " 27: 'can',\n",
       " 28: 'water',\n",
       " 29: 'sour',\n",
       " 30: 'is',\n",
       " 31: 'rat',\n",
       " 32: 'A',\n",
       " 33: 'and',\n",
       " 34: 'rough',\n",
       " 35: 'fruit',\n",
       " 36: 'smooth',\n",
       " 37: 'Rat',\n",
       " 38: 'home',\n",
       " 39: '<UNK>'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word = {v:k for k, v in word_2_idx.items()}\n",
    "\n",
    "index2word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Prepare data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgrams = []\n",
    "\n",
    "for sent in corpus_tokenized :\n",
    "    # print()\n",
    "    for i in range(2, len(sent) - 2): #start from 1 to second last\n",
    "        center_word = sent[i]\n",
    "        outside_words = [word_2_idx[sent[i-2]], word_2_idx[sent[i-1]], word_2_idx[sent[i+1]], word_2_idx[sent[i+2]]]  #window_size = 1\n",
    "        for o in outside_words:\n",
    "            skipgrams.append([center_word, o])\n",
    "\n",
    "skipgrams\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBOW = []\n",
    "\n",
    "    #for each corpus\n",
    "for sent in corpus_tokenized:\n",
    "    #for each sent [\"apple\", \"banana\", \"fruit\"]\n",
    "    for i in range(2, len(sent) - 2): #start from 2 to second last\n",
    "        center_word = word_2_idx[sent[i]]\n",
    "        outside_words = [word_2_idx[sent[i-2]], word_2_idx[sent[i-1]], word_2_idx[sent[i+1]], word_2_idx[sent[i+2]]]  #window_size = 2\n",
    "        for o in outside_words:\n",
    "            CBOW.append([o , center_word])\n",
    "print(CBOW)     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### real function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, corpus):\n",
    "    \n",
    "    skipgrams = []\n",
    "\n",
    "    #for each corpus\n",
    "    for sent in corpus_tokenized:\n",
    "        #for each sent [\"apple\", \"banana\", \"fruit\"]\n",
    "        for i in range(2, len(sent) - 2): #start from 1 to second last\n",
    "            center_word = word_2_idx[sent[i]]\n",
    "            outside_words = [word_2_idx[sent[i-2]], word_2_idx[sent[i-1]], word_2_idx[sent[i+1]], word_2_idx[sent[i-2]]]  #window_size = 1\n",
    "            for o in outside_words:\n",
    "                skipgrams.append([center_word, o])\n",
    "    # print(skipgrams)\n",
    "    # print(skipgrams)            \n",
    "    #only get a batch, not the entire list\n",
    "    random_inputs, random_labels = [], []   \n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "             \n",
    "    #appending some list of inputs and labels\n",
    "    \n",
    "    for index in random_index:\n",
    "        random_inputs.append([skipgrams[index][0]])  #center words, this will be a shape of (1, ) --> (1, 1) for modeling\n",
    "        random_labels.append([skipgrams[index][1]])\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, word_sequence):\n",
    "    \n",
    "    # Make skip gram of one size window\n",
    "    skip_grams = []\n",
    "    # loop each word sequence\n",
    "    # we starts from 1 because 0 has no context\n",
    "    # we stop at second last for the same reason\n",
    "    for sent in corpus:\n",
    "        for i in range(1, len(sent) - 1):\n",
    "            target = word_2_idx[sent[i]]\n",
    "            context = [word_2_idx[sent[i - 1]], word_2_idx[sent[i + 1]]]\n",
    "            for w in context:\n",
    "                skip_grams.append([target, w])\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([skip_grams[i][1]])  # context word, e.g., 3\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remake CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, corpus):\n",
    "    \n",
    "    CBOW_data = []\n",
    "\n",
    "    #for each corpus\n",
    "    for sent in corpus_tokenized:\n",
    "        #for each sent [\"apple\", \"banana\", \"fruit\"]\n",
    "        for i in range(2, len(sent) - 2): #start from 1 to second last\n",
    "            center_word = word_2_idx[sent[i]]\n",
    "            outside_words = [word_2_idx[sent[i-2]], word_2_idx[sent[i-1]], word_2_idx[sent[i+1]], word_2_idx[sent[i-2]]]  #window_size = 1\n",
    "            for o in outside_words:\n",
    "                CBOW_data.append([o, center_word])\n",
    "    # print(CBOW_data)\n",
    "    # print(skipgrams)            \n",
    "    #only get a batch, not the entire list\n",
    "    random_index = np.random.choice(range(len(CBOW_data)), batch_size, replace=False)\n",
    "             \n",
    "    #appending some list of inputs and labels\n",
    "    random_inputs, random_labels = [], []   \n",
    "    for index in random_index:\n",
    "        random_inputs.append([CBOW_data[index][0]])  #center words, this will be a shape of (1, ) --> (1, 1) for modeling\n",
    "        random_labels.append([CBOW_data[index][1]])\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, corpus):\n",
    "    cbow_data = []\n",
    "\n",
    "    #for each corpus\n",
    "    for sent in corpus_tokenized:\n",
    "        #for each sent [\"apple\", \"banana\", \"fruit\"]\n",
    "        for i in range(2, len(sent) - 2): #start from 2 to second last\n",
    "            center_word = word_2_idx[sent[i]]\n",
    "            outside_words = [word_2_idx[sent[i-2]], word_2_idx[sent[i-1]], word_2_idx[sent[i+1]], word_2_idx[sent[i+2]]]  #window_size = 2\n",
    "            cbow_data.append((outside_words, center_word))\n",
    "    print(cbow_data)            \n",
    "    #only get a batch, not the entire list\n",
    "    random_index = np.random.choice(range(len(cbow_data)), batch_size, replace=False)\n",
    "             \n",
    "    #appending some list of inputs and labels\n",
    "    random_inputs, random_labels = [], []   \n",
    "    for index in random_index:\n",
    "        random_inputs.append(cbow_data[index][0])  #outside words, this will be a shape of (4, ) --> (4, 1) for modeling\n",
    "        random_labels.append(cbow_data[index][1])\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "label=array([[26],\n",
      "       [ 7]])\n"
     ]
    }
   ],
   "source": [
    "input, label = random_batch(2, corpus_tokenized)\n",
    "# print(random_batch(10, corpus_tokenized))\n",
    "print(f\"{input.shape}\")\n",
    "print(f\"{label=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size = len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mango',\n",
       " 'Fish',\n",
       " 'live',\n",
       " 'eat',\n",
       " 'under',\n",
       " 'the',\n",
       " 'to',\n",
       " 'animal',\n",
       " 'everywhere,',\n",
       " 'taste',\n",
       " 'has',\n",
       " 'river,',\n",
       " 'The',\n",
       " 'sea',\n",
       " 'it',\n",
       " 'An',\n",
       " 'trash',\n",
       " 'cat',\n",
       " 'orange',\n",
       " 'grape',\n",
       " 'skin',\n",
       " 'like',\n",
       " 'or',\n",
       " 'breath',\n",
       " 'sweet',\n",
       " 'in',\n",
       " 'that',\n",
       " 'can',\n",
       " 'water',\n",
       " 'sour',\n",
       " 'is',\n",
       " 'rat',\n",
       " 'A',\n",
       " 'and',\n",
       " 'rough',\n",
       " 'fruit',\n",
       " 'smooth',\n",
       " 'Rat',\n",
       " 'home',\n",
       " '<UNK>']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center_word  = nn.Embedding(voc_size, emb_size)  #is a lookup table mapping all ids in voc_size, into some vector of size emb_size\n",
    "        self.embedding_outside_word = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center_word, outside_word, all_vocabs):\n",
    "        #center_word, outside_word: (batch_size, 1)\n",
    "        #all_vocabs: (batch_size, voc_size)\n",
    "        \n",
    "        #convert them into embedding\n",
    "        center_word_embed  = self.embedding_center_word(center_word)     #(batch_size, 1, emb_size)\n",
    "        outside_word_embed = self.embedding_outside_word(outside_word)   #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embed   = self.embedding_outside_word(all_vocabs)     #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        #bmm is basically @ or .dot , but across batches (i.e., ignore the batch dimension)\n",
    "        top_term = outside_word_embed.bmm(center_word_embed.transpose(1, 2)).squeeze(2)\n",
    "        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) ===> (batch_size, 1)\n",
    "        \n",
    "        top_term_exp = torch.exp(top_term)  #exp(uo vc)\n",
    "        #(batch_size, 1)\n",
    "        \n",
    "        lower_term = all_vocabs_embed.bmm(center_word_embed.transpose(1, 2)).squeeze(2)\n",
    "         #(batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size)\n",
    "         \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1) #sum exp(uw vc)\n",
    "        #(batch_size, 1)\n",
    "        \n",
    "        loss_fn = -torch.mean(torch.log(top_term_exp / lower_term_sum))\n",
    "        #(batch_size, 1) / (batch_size, 1) ==mean==> scalar\n",
    "        \n",
    "        return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embedding = nn.Embedding(voc_size, emb_size)  #is a lookup table mapping all ids in voc_size, into some vector of size emb_size\n",
    "    \n",
    "    def forward(self, outside_words, center_word):\n",
    "        #outside_words: (batch_size, window_size*2, 1)\n",
    "        #center_word: (batch_size, 1)\n",
    "        \n",
    "        outside_words_embed = self.embedding(outside_words.squeeze())    #(batch_size, window_size*2, emb_size)\n",
    "        center_word_embed = self.embedding(center_word)        #(batch_size, 1, emb_size)\n",
    "        \n",
    "        #sum up the embeddings of context words\n",
    "        outside_words_embed_sum = torch.sum(outside_words_embed, 1) #(batch_size, emb_size)\n",
    "        \n",
    "        #bmm is basically @ or .dot , but across batches (i.e., ignore the batch dimension)\n",
    "        top_term = center_word_embed.bmm(outside_words_embed_sum.unsqueeze(2)).squeeze(2)\n",
    "        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) ===> (batch_size, 1)\n",
    "        \n",
    "        top_term_exp = torch.exp(top_term)  #exp(uo vc)\n",
    "        #(batch_size, 1)\n",
    "        \n",
    "        loss_fn = -torch.mean(torch.log(top_term_exp))\n",
    "        #(batch_size, 1) ==mean==> scalar\n",
    "        \n",
    "        return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 40])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    #map(function, list of something)\n",
    "    #map will look at each of element in this list, and apply this function\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word_2_idx).expand(batch_size, voc_size)\n",
    "all_vocabs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33],\n",
       "       [26]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, label = random_batch(batch_size, corpus_tokenized)\n",
    "input #center word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 2 #usually, this can be 50, 100, or 300\n",
    "model = Skipgram(voc_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 2 #usually, this can be 50, 100, or 300\n",
    "model = CBOW(voc_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.LongTensor(input)  \n",
    "label_tensor = torch.LongTensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(input_tensor, label_tensor, all_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6310, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 #why?  no reason; \n",
    "emb_size   = 2 #why?  no reason; usually 50, 100, 300, but 2 so we can plot (50 can also plot, but need PCA)\n",
    "model      = Skipgram(voc_size, emb_size)\n",
    "\n",
    "criterion  = nn.CrossEntropyLoss()  #-log\n",
    "optimizer  = optim.Adam(model.parameters(), lr=0.0009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 | Loss: 3.502455 | Time: ??\n",
      "Epoch 2000 | Loss: 2.886886 | Time: ??\n",
      "Epoch 3000 | Loss: 1.346872 | Time: ??\n",
      "Epoch 4000 | Loss: 3.091658 | Time: ??\n",
      "Epoch 5000 | Loss: 1.583554 | Time: ??\n",
      "Epoch 6000 | Loss: 4.813123 | Time: ??\n",
      "Epoch 7000 | Loss: 1.397553 | Time: ??\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 7000\n",
    "#for epoch\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #get random batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "    \n",
    "    # print(input_batch.shape, label_batch.shape, all_vocabs.shape)\n",
    "    \n",
    "    #loss = model\n",
    "    loss = model(input_batch, label_batch, all_vocabs)\n",
    "    \n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print epoch loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.6f} | Time: ??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(word):\n",
    "    try:\n",
    "        index = word_2_idx[word]\n",
    "    except:\n",
    "        index = word_2_idx['<UNK>']\n",
    "    \n",
    "    word = torch.LongTensor([index])\n",
    "\n",
    "    center_embed  = model.embedding_center_word(word)\n",
    "    outside_embed = model.embedding_outside_word(word)\n",
    "    \n",
    "    embed = (center_embed + outside_embed) / 2\n",
    "    \n",
    "    return  embed[0][0].item(), embed[0][1].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,20))\n",
    "for i, word in enumerate(vocabs[:20]): #loop each unique vocab\n",
    "    x, y = get_embed(word)\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat          = get_embed('cat')\n",
    "\n",
    "smooth        = get_embed('smooth')\n",
    "mango = get_embed('mango')\n",
    "animal       = get_embed('animal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_senario(a1 , a2) :\n",
    "    tester1 = get_embed(a1)\n",
    "    teater2 = get_embed(a2)\n",
    "    print(f\"{a1} vs {a2} = {cos_sim(tester1 , teater2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.341585636138916, 2.1411867141723633)"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim\n",
    "    \n",
    "# print(f\"cat vs. smooth: \",        cos_sim(cat, smooth))\n",
    "# print(f\"mango vs. animal: \",       cos_sim(mango, animal))\n",
    "# print(f\"mango vs. smooth: \",       cos_sim(mango, smooth))\n",
    "# print(f\"cat vs. cat: \",          cos_sim(cat, cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat vs fruit = 0.9953290568145609\n"
     ]
    }
   ],
   "source": [
    "test_senario(\"cat\",\"fruit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3dc66c11351b0102d9b13bb868253b81f2002b43c320e9088049bbc4e6917ed6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
