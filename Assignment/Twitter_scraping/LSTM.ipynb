{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Part 3: TorchText + Padded + BiLSTM\n",
    "\n",
    "\n",
    "Here we shall improve the previous one by adding a LSTM.  A bidirectional multilayer one.\n",
    "\n",
    "Standard RNNs suffer from the vanishing gradient problem. LSTMs overcome this by having an extra recurrent state called a _cell_, $c$ - which can be thought of as the \"memory\" of the LSTM - and the use use multiple _gates_ which control the flow of information into and out of the memory. \n",
    "\n",
    "$$(h_t, c_t) = \\text{LSTM}(x_t, h_t, c_t)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "i_t &= \\sigma(\\mathbf{W}_{ii}x_t + b_{ii} + \\mathbf{W}_{hi}h_{t-1} + b_{hi}) \\\\\n",
    "f_t &= \\sigma(\\mathbf{W}_{if}x_t + b_{if} + \\mathbf{W}_{hf}h_{t-1} + b_{hf})\\\\\n",
    "g_t &= \\text{tanh}(\\mathbf{W}_{ig}x_t + b_{ig} + \\mathbf{W}_{hg}h_{t-1} + b_{hg})\\\\\n",
    "o_t &= \\sigma(\\mathbf{W}_{io}x_t + b_{io} + \\mathbf{W}_{ho}h_{t-1} + b_{ho}) \\\\\n",
    "c_t &= f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "h_t &= o_t \\odot \\text{tanh}(c_t)\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus, the model using an LSTM looks something like:\n",
    "\n",
    "<img src=\"figures/sentiment2.png\" width=\"350\">\n",
    "\n",
    "The initial cell state, $c_0$, like the initial hidden state is initialized to a tensor of all zeros. The sentiment prediction is still, however, only made using the final hidden state, not the final cell state, i.e. $\\hat{y}=f(h_T)$.\n",
    "\n",
    "In implementation, to use an LSTM instead of the standard RNN, we use `nn.LSTM` instead of `nn.RNN`. Also, note that the LSTM returns the `output` and a tuple of the final `hidden` state and the final `cell` state, whereas the standard RNN only returned the `output` and final `hidden` state. \n",
    "\n",
    "### Bidirectional RNN\n",
    "\n",
    "The concept behind a bidirectional RNN is simple. As well as having an RNN processing the words in the sentence from the first to the last (a forward RNN), we have a second RNN processing the words in the sentence from the **last to the first** (a backward RNN). At time step $t$, the forward RNN is processing word $x_t$, and the backward RNN is processing word $x_{T-t+1}$. \n",
    "\n",
    "In PyTorch, the hidden state (and cell state) tensors returned by the forward and backward RNNs are stacked on top of each other in a single tensor. We make our sentiment prediction using a concatenation of the last hidden state from the forward RNN (obtained from final word of the sentence), $h_T^\\rightarrow$, and the last hidden state from the backward RNN (obtained from the first word of the sentence), $h_T^\\leftarrow$, i.e. $\\hat{y}=f(h_T^\\rightarrow, h_T^\\leftarrow)$   \n",
    "\n",
    "In implementation, the final hidden state, `hidden`, has a shape of _**[num layers * num directions, batch size, hid dim]**_. These are ordered: **[forward_layer_0, backward_layer_0, forward_layer_1, backward_layer 1, ..., forward_layer_n, backward_layer n]**. As we want the final (top) layer forward and backward hidden states, we get the top two hidden layers from the first dimension, `hidden[-2,:,:]` and `hidden[-1,:,:]`, and concatenate them together before passing them to the linear layer. \n",
    "\n",
    "Also, as the final hidden state of our LSTM has both a forward and a backward component, which will be concatenated together, the size of the input to the `nn.Linear` layer is twice that of the hidden dimension size.\n",
    "\n",
    "The image below shows a bi-directional RNN, with the forward RNN in orange, the backward RNN in green and the linear layer in silver.  \n",
    "\n",
    "<img src = \"figures/sentiment3.png\" width=\"300\">\n",
    "\n",
    "### Multi-layer RNN\n",
    "\n",
    "Multi-layer RNNs (also called *deep RNNs*) are another simple concept. The idea is that we add additional RNNs on top of the initial standard RNN, where each RNN added is another *layer*. The hidden state output by the first (bottom) RNN at time-step $t$ will be the input to the RNN above it at time step $t$. The prediction is then made from the final hidden state of the final (highest) layer.\n",
    "\n",
    "The image below shows a multi-layer unidirectional RNN, where the layer number is given as a superscript. Also note that each layer needs their own initial hidden state, $h_0^L$.\n",
    "\n",
    "<img src = \"figures/sentiment4.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@leostsantos Se fosse só uma, mas vem a pilha toda kkk\n",
      "@Leydicruz_ Balbaroooo pero yo toy Feli por el Hogwarts ta demasiado dura\n",
      "@UnmodernM Hogwarts Legacy is set in the late 1800s 😂\n",
      "@QB1TATT00 Bro it’s Thursday, you been too deep into Hogwarts Legacy you lost track of time lol\n",
      "@xMaRk1988 el hogwarts lo tienes finiqitado ya???\n",
      "Jisoo quelle femme\n",
      "What's your hogwarts house?\n",
      "@RosaMajuSant MAJU!! Eu tô apaixonado, é a coisa mais linda ficar vagando por Hogwarts e arredores.  Tudo bem que as vezes tem um bugzinho ou outro tipo esse de cair do mapa, mas meu deus que coisa legal\n",
      "@MisterPxl Je l’ai annoncé dès mon grand test : oui, Hogwarts Legacy a toutes les chances d’être dans les nommés pour le GOTY !!! Pour la victoire, attendons de voir la qualité des autres potentiels prétendants…  Une seule certitude : 2023, année de FOU 🥰\n",
      "Ravenclaw! Wizardry Continues in hogwarts legacy! :) | Join in!  https://t.co/VRwueMTVHZ  #twitchstreamer #twitch #twitchpartner #streamer #twitchaffiliate #gaming @OwlRetweets @OwlShare @BlazedRTs @StreamersRT1 @TeamYouTube @YouTubeCreators #hogwarts #hogwartslegacy\n",
      "@StoolGreenie Hit the climbing gym and play hogwarts legacy\n",
      "Allez à 14h je lance un live sur Hogwarts Legacy parce que je veux absolument avancer dans l’histoire ! 🐍🧙‍♂️   https://t.co/JwEuviPw6H\n",
      "@JezCorden No chance for hogwarts or starfield.Zelda will win.\n",
      "@YRGNews @YourRAGEz Mad he ain’t touched hogwarts after his first time playing 😂\n",
      "Se revela cuál es la Casa preferida por los jugadores de Hogwarts Legacy.  Más &gt;&gt;&gt;  https://t.co/zMwSYCNTd0  https://t.co/ZFYgEcJUBf\n",
      "potrzebuje hogwarts legacy naprawde. kup mi ktos.\n",
      "Les idols selon @ fionha qd ils voient un lightstick d’un autre groupe  https://t.co/tANtJEMEnx\n",
      "Eu tirei: Remus! E você?  https://t.co/xOmczl1JtX #marotos #jkRowling #sagas #hp #hogwarts #harryPotter #livros #filmes via @Quizur_PT\n",
      "輸入になるので値段変動ありますが、1番安い価格での提供になるそうです！\n",
      "🐻I'm now streaming👉Hogwarts Legacy(February 16, 2023 at 08:53PM)  https://t.co/15oNfS9Fyk\n",
      "Hogwarts Legacy is grafisch prachtig, maar de story, dialoog en sidequests zijn echt teleurstellend saai.  Ik denk niet dat het kans heeft op GOTY, zeker niet aangezien Tears of the Kingdom en Starfield op de loer liggen.\n",
      "@girlsromantics e o hogwarts legacy?? como você ta indo nele?\n",
      "Hogwarts Legacy has streamed now.\n",
      "Mein Kollege hat verbotene Wald von Hogwarts im Sack 💀\n",
      "I am LIVE! 2. Run auf schwer mit neuem Haus startet jetzt!  https://t.co/IIsk3IYjfe #HogwartsLegacy #ChillVibes #Hogwarts #HogwartsLegacyMM #TwitchStreamers #Livestream #SupportSmallStreamers #smallstreamers #LiveStreaming #survival\n",
      "@xboxphysical @Xbox @XboxP3 @BondSarah_Bond @iocat @majornelson @aarongreenberg @JezCorden @jronald I'm afraid it's worse than that. Xbox is intentionally killing its physical gamer community. There are reports that will no longer sell physical games in some countries like Brazil. The physical Xbox version of Hogwarts Legacy wasn't released in Brazil.    https://t.co/BLstbMUKx2\n",
      "@GRCinemaTicket The Crow\n",
      "What Governmental Body Directly Preceded the Ministry of Magic? Hogwarts Legacy Answers  https://t.co/CL5qopzjnI #EddiesGamingNews #EddiesGaming #GamingNews\n",
      "Earned 1 PSN trophy (1 bronze) in Hogwarts Legacy  https://t.co/XwcKqlxDWM\n",
      "Earned 1 PSN trophy (1 silver) in Hogwarts Legacy  https://t.co/L8HTHe0gyD\n",
      "@lukinhapc @Keymailer UHAEUHAUEHAUEHAUEHAUHE sim\n",
      "@TheDazeel Banger year so far no doubt. I'd include Hogwarts for those enjoying and Age Of Empires 2 on console for myself as well. (its a great port, works shockingly well with controller.)\n",
      "Bon, je suis assez mitigée sur le jeu Hogwarts Legacy… for so many reasons… vous en pensez quoi vous ? #SpoilerAlert\n",
      "Que shooow todo la verdad y yo mientras tanto con mi hogwarts legacy y mi pibe Sebastian\n",
      "Amazonでは英語版ですがホグワーツレガシーのアートブックを購入できます！文章読めるか不安という方はRTしている画像などをご覧下さい！   https://t.co/Ish14Jv6Xx   #HogwartsLegacy #ホグワーツレガシー\n",
      "Hogwarts Legacy is really awesome, btw. I just wish I had more time to play last night.\n",
      "Primer video de Hogwarts Legacy!!   ✅MISION PRINCIPAL✅ / 🧙LEVIOSO🧙 / 📕Clase de encantamientos📕  https://t.co/UfDByZdnaN a través de @YouTube\n",
      "The Podcast of the Lotus Eaters #591  @Sargon_of_Akkad and Callum discuss Hogwarts Legacy: End Times, The Game Was Rigged From the Start, and Vice Is Tired of Taking Ls IRL.   https://t.co/sjfkpcrL1R  LIVE 1PM GMT\n",
      "What happens when you visit Hogsmeade with Sebastian or Natsai (Both choices &amp; outcomes) - Hogwarts Legacy #HarryPotter #Poudlard #Merlin #Magic #Wizard #PreAuLard #Dumbledore #Snape #Voldemort #AvadaKedavra #Snape #LetsPlay #Rpg   https://t.co/lxX2otk1Q0\n",
      "@odeirne kkk tu já terminou hogwarts?\n",
      "Переводимо ночами Hogwarts Legacy, щоб я могла в неї грати 💜💜\n",
      "@jk_rowling  absolutely love Hogwarts Legacy! Friday nights, sorted. Thanks 😊\n",
      "OmegaGamesWikiさん配信中  【ホグワーツ・レガシー】Hogwarts Legacy #17  サイドクエスト  https://t.co/KZ6zMzaPFG  🧙‍♀️🧹 ⏯️再生リスト ↓  https://t.co/65w1bveK7N  🎮OmegaGamesWiki  https://t.co/grwiJ7erbY  https://t.co/wFwYAuwtgy\n",
      "Liat hogwarts legacy jadi gasabar pengen baca harpot, pengen liat pesona ginny dan ron, katanya ron lebih ada andil ini itu di bukunya terus hermione lebih kutu buku daripada di pilem, anna watson kecakepan sih\n",
      "@Zefyrtheimp @Abstrac70303991 @Flutternutter3 No I actually know someone who plays hogwarts legacy and their not Transphobic.\n",
      "Arrêtez de chercher des mutus Kpop stan y a que ça dans ma tl 😡\n",
      "🔴LIVE🔴 HOGWARTS LEGACY!  Come check in and support if you can! ❤️   https://t.co/wT1U7CJH6Y  #SupportSmallStreamers #SupportSmallStreams #SmallStreamerCommunity #SmallStreamersConnect #smallstreamer #smallyoutuber #smallyoutubercommunity #HogwartsLegacy #hogwarts\n",
      "watching hogwarts legacy gameplay just makes me wanna play elden ring and not this\n",
      "Okay Hogwarts Legacy is sick\n",
      "@rightofcenter73 @TransTerri Should’ve kept fucking scrolling. The fact that you have the # IStandWithJKRowling in your profile also, makes me want to play Hogwarts Legacy today. But that probably incites hatred too 🙄\n",
      "@captmarlv @tanyarlfes foom tu dingin bgt ga si\n",
      "Eu n peguei nenhum spoiler de hogwarts legacy (sim, eu consegui) e eu nunca fui tão feliz, fico me surpreendendo com cada coisinha\n",
      "Pré-venda de Hogwarts Legacy via Amazon🇧🇷  De R$299 por R$285 + frete grátis ou em até 5x de R$ 57,23 sem juros - Versão para Playstation 4  Acesse  https://t.co/zonwRHFdfh  https://t.co/CQpqK9dVhN\n",
      "hoy se sigue con el hogwarts gente\n",
      "Buen día para regalar hogwarts legacy\n",
      "straight outta hogwarts 💀  https://t.co/eIb8yQzkMF\n",
      "Queria tá jogando Hogwarts Legacy, mas tenho que estudar…\n",
      "@CenayangFilm \"Harry Potter gimana? Lu, lu mau expect liat film Harry Potter asli Hogwarts, ya kagak bisa, Narnia gimane anjing?!\"\n"
     ]
    }
   ],
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "# need this if run in ipynb\n",
    "nest_asyncio.apply() #\n",
    "c = twint.Config()\n",
    "c.Search = \"hogwarts\"\n",
    "c.Limit = 50\n",
    "c.Store_object = True\n",
    "c.Hide_output = True # don't show output in terminal\n",
    "c.Lang = \"english\" # english\n",
    "twint.run.Search(c)\n",
    "\n",
    "tweets = twint.output.tweets_list\n",
    "\n",
    "\n",
    "for tw in tweets:\n",
    "\n",
    "    print(tw.tweet) # str\n",
    "tweets.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ETL: Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this if you are not using our department puffer\n",
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "from torchtext.datasets import SST2\n",
    "train,valid, _ = SST2()\n",
    "\n",
    "# from torchtext.datasets import AG_NEWS\n",
    "# _, test = AG_NEWS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShardingFilterIterDataPipe"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so this is a datapipe object; very similar to pytorch dataset version 2 which is better\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hide new secretions from the parental units', 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at one example of train\n",
    "next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's change the unique label\n",
    "# set([y for y, x in list(iter(train))])\n",
    "#{“World”, “Sports”, “Business”, “Sci/Tech”}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67349"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hide new secretions from the parental units', 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 120000 gonna just take up too much of our GPU, and also for the sake of learning, we gonna resize it.....  All `DataPipe` instance has a handy function called `random_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, nono = train.random_split(total_length=train_size, weights = {\"train\": 0.35, \"test\": 0.05, \"nono\": 0.1}, seed=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47144"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "872"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = len(list(iter(valid)))\n",
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6735"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = len(list(iter(test)))\n",
    "test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "The first step is to decide which tokenizer we want to use, which depicts how we split our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'are', 'learning', 'torchtext', 'in', 'AIT', '!']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install spacy\n",
    "#python3 -m spacy download en_core_web_sm\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "tokens = tokenizer(\"We are learning torchtext in AIT!\")  #some test\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to integers (numeral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train), specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[245, 13, 6, 0, 6]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab(['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gorgeous'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab.get_itos()\n",
    "\n",
    "#print 159, for example\n",
    "mapping[509]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "vocab(['dddd', 'aaaa'])\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "vocab(['<pad>', '<bos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13666"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so we can 50k+ unique vocabularies!\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FastText Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText\n",
    "fast_vectors = FastText(language='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab.get_itos()).to(device)\n",
    "# vocab.get_itos() returns a list of strings (tokens), where the token at the i'th position is what you get from doing vocab[token]\n",
    "# get_vecs_by_tokens gets the pre-trained vector for each string when given a list of strings\n",
    "# therefore pretrained_embedding is a fully \"aligned\" embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13666, 300])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_embedding.shape   #we have X vocabs, each with a 300 fasttext embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preparing the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In torchtext, first thing before the batch iterator is to define how you want to process your text and label.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline  = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) #turn {1, 2, 3, 4} to {0, 1, 2, 3} for pytorch training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at example how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 88, 10, 512, 6162]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline(\"I love to play football\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline('1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's make the batch iterator.  Here we create a function <code>collate_fn</code> that define how we want to create our batch.  **We gonna add length of the sequence since packed padded sequences require this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data   import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "pad_idx = vocab['<pad>'] #++<----making sure our embedding layer ignores pad\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_text, _label) in batch:\n",
    "        # print(_text)\n",
    "        # print(_label)\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        length_list.append(processed_text.size(0))  #++<-----packed padded sequences require \n",
    "    #criterion expects float labels\n",
    "    return torch.tensor(label_list, dtype=torch.int64), pad_sequence(text_list,  padding_value=pad_idx, batch_first=True), torch.tensor(length_list, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "train_loader = DataLoader(train, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test, batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47144"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(collate_batch(train)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('putters along looking for astute observations and coming up blank', 0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader.  Note that {“World”, “Sports”, “Business”, “Sci/Tech”} maps to {0, 1, 2, 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, text, length in train_loader:\n",
    "    # print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0,  ..., 0, 1, 0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label shape:  torch.Size([1024])\n",
      "Text shape:  torch.Size([1024, 48])\n"
     ]
    }
   ],
   "source": [
    "print(\"Label shape: \", label.shape) # (batch_size, )\n",
    "print(\"Text shape: \", text.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Design the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        #put padding_idx so asking the embedding layer to ignore padding\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(emb_dim, \n",
    "                           hid_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hid_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        #text = [batch size, seq len]\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #++ pack sequence ++\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False, batch_first=True)\n",
    "        \n",
    "        #embedded = [batch size, seq len, embed dim]\n",
    "        packed_output, (hn, cn) = self.lstm(packed_embedded)  #if no h0, all zeroes\n",
    "        \n",
    "        #++ unpack in case we need to use it ++\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        #output = [batch size, seq len, hidden dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        #hn = [batch size, hidden dim * num directions]\n",
    "        \n",
    "        return self.fc(hn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A bit about dropout\n",
    "\n",
    "Dropout is a regularization technique.  During training, dropout randomly sets units in the hidden layer $\\mathbf{h}$ to zero with probabilty $p_{\\text{drop}}$ (dropping different units each minibatch), and then multiplies $\\mathbf{h}$ by a constant $\\gamma$.  We can write this as:\n",
    "\n",
    "$$\\mathbf{h}_{\\text{drop}} = \\gamma \\mathbf{d} \\circ \\mathbf{h}$$\n",
    "\n",
    "where $\\mathbf{d} \\in \\{0, 1\\}^{D_h}$ ($D_h$ is the size of $\\mathbf{h}$) is a mask vector where each entry is 0 with probability $p_{\\text{drop}}$ and 1 with probability ($1 - p_{\\text{drop}}$). For the gamma constant, $\\gamma$ is chosen such that the expected value of $\\mathbf{h}_{\\text{drop}}$ is $\\mathbf{h}$ \n",
    "\n",
    "$$\\mathbb{E}_{\\text{pdrop}}[\\mathbf{h}_{\\text{drop}}]_i = h_i$$\n",
    "\n",
    "for all $i \\in \\{1, \\cdots, D_h\\}$\n",
    "\n",
    "During training we drop units at a rate of $p_{\\text{drop}}$, resulting in roughly $p_{\\text{keep}} = 1 - p_{\\text{drop}}$ fraction of units left over. At test time we’d like to have the effect of keeping a similar fraction, $p_{\\text{keep}}$, of units on. By scaling down the layer units by $γ = \\frac{1}{1-p_{\\text{drop}}} = \\frac{1}{p_{\\text{keep}}}$, we effectively level out their magnitudes so that both phases of learning share very similar expected outputs.\n",
    "\n",
    "The goal of dropout is to reduce overfitting. We’re interested in updating unit weights so as to form a network that performs well across different datasets. Now, during evaluation we’re concerned with how well the model handles unseen data. When we dropout units, we’re “thinning” out the network which in many cases will add noise to predictions and dampen accuracy. Thus, if we were to apply dropout during evaluation time, we would not be able to fairly assess the generalization power of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explicitly initialize weights for better learning\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim  = len(vocab)\n",
    "hid_dim    = 256\n",
    "emb_dim    = 300         #**<----change to 300\n",
    "output_dim = 2 #four classes\n",
    "\n",
    "#for biLSTM\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "\n",
    "model = LSTM(input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout).to(device)\n",
    "model.apply(initialize_weights)\n",
    "model.embedding.weight.data = fast_embedding #**<------applied the fast text embedding as the initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4099800\n",
      "307200\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "307200\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "524288\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "524288\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "  1024\n",
      "     2\n",
      "______\n",
      "6820570\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr=1e-3\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss() #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, y):\n",
    "    \n",
    "    predicted = torch.max(preds.data, 1)[1]\n",
    "    batch_corr = (predicted == y).sum()\n",
    "    acc = batch_corr / len(y)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, loader_length):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train() #useful for batchnorm and dropout\n",
    "    \n",
    "    for i, (label, text, text_length) in enumerate(loader): \n",
    "        # print(text)\n",
    "        label = label.to(device) #(batch_size, )\n",
    "        text = text.to(device) #(batch_size, seq len)\n",
    "                \n",
    "        #predict\n",
    "        predictions = model(text, text_length).squeeze(1) #output by the fc is (batch_size, 1), thus need to remove this 1\n",
    "        \n",
    "        #calculate loss\n",
    "        loss = criterion(predictions, label)\n",
    "        acc  = accuracy(predictions, label)\n",
    "        \n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "                        \n",
    "    return epoch_loss / loader_length, epoch_acc / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (text, label, text_length) in enumerate(train_loader): \n",
    "#         print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0,  ..., 0, 1, 0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (label, text, text_length) in enumerate(loader): \n",
    "            label = label.to(device) #(batch_size, )\n",
    "            text  = text.to(device)  #(seq len, batch_size)\n",
    "\n",
    "            predictions = model(text, text_length).squeeze(1) \n",
    "            \n",
    "            loss = criterion(predictions, label)\n",
    "            acc  = accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / loader_length, epoch_acc / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15556\\3808519285.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15556\\2457109905.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, loader, optimizer, criterion, loader_length)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m#backprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\earth\\anaconda3\\envs\\project\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[0;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         )\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\earth\\anaconda3\\envs\\project\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs      = 20\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, train_loader_length)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_accs.append(valid_acc)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!  Very high accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_accs, label = 'train acc')\n",
    "ax.plot(valid_accs, label = 'valid acc')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"Google is now falling nonstop.  The price is really bad now.\"\n",
    "text = torch.tensor(text_pipeline(test_str)).to(device)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [x.item() for x in text]\n",
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mapping[num] for num in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([text.size(1)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, text_length):\n",
    "    with torch.no_grad():\n",
    "        output = model(text, text_length).squeeze(1)\n",
    "        predicted = torch.max(output.data, 1)[1]\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(text, text_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In general, LSTM provides a very strong baseline for all kind of sequential data.  Anyhow, LSTM still suffer when the sequence is very long (500+).  In that case, we may want to use convolution instead.  In the next tutorial, we shall take a look."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03c0b7698bc8ddcbc4ac2f5708958d09d9f2028b88f43ee7158b288fbdf68079"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
