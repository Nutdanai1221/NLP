{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzmYMVWGDE4"
      },
      "source": [
        "# Recurrent Neural Networks and Language Models\n",
        "\n",
        "You guys probably very excited about ChatGPT.  In today class, we will be implementing a very simple language model, which is basically what ChatGPT is, but with a simple LSTM.  You will be surprised that it is not so difficult at all.\n",
        "\n",
        "Paper that we base on is *Regularizing and Optimizing LSTM Language Models*, https://arxiv.org/abs/1708.02182"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kvrKIY3OGGnX"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BGBitb-UGDE-",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext, datasets, math\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibJD2xpAGDE_",
        "outputId": "58126ca7-4c07-43ac-857c-627f71f2c3fd",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "#make our work comparable if restarted the kernel\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p6xpeC7wAuY_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pythainlp\n",
            "  Downloading pythainlp-3.1.1-py3-none-any.whl (9.6 MB)\n",
            "Requirement already satisfied: requests>=2.22.0 in c:\\users\\earth\\anaconda3\\envs\\project\\lib\\site-packages (from pythainlp) (2.28.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\earth\\anaconda3\\envs\\project\\lib\\site-packages (from requests>=2.22.0->pythainlp) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\earth\\anaconda3\\envs\\project\\lib\\site-packages (from requests>=2.22.0->pythainlp) (1.26.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\earth\\anaconda3\\envs\\project\\lib\\site-packages (from requests>=2.22.0->pythainlp) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\earth\\anaconda3\\envs\\project\\lib\\site-packages (from requests>=2.22.0->pythainlp) (2022.12.7)\n",
            "Installing collected packages: pythainlp\n",
            "Successfully installed pythainlp-3.1.1\n"
          ]
        }
      ],
      "source": [
        "! pip install pythainlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CasDuAWTA2zI"
      },
      "outputs": [],
      "source": [
        "from pythainlp.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzVLDAo-GDFA"
      },
      "source": [
        "## 1. Load data - Thaisum\n",
        "We will be using wikitext which contains a large corpus of text, perfect for language modeling task.  This time, we will use the `datasets` library from HuggingFace to load."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "877284d0f1cc48c88b62187ebd4913a6",
            "cf6d2b43256a47f68490e2b91b4751d1",
            "d6db6d415a9b41a6989b3558767f75bf",
            "415c30dc5fcf4790bca17e9124fa6bd1",
            "2ddf5268bb214f7a9bae9bec1781e547",
            "f6d6dc065dac4a9db5ed37f58506bc62",
            "f4417c7459e74c3a88cd20a7dbd85fd3",
            "a5aac3a0b556497694bc5f4e8a34514a",
            "f6fd3468f5854e61a956af740efddbc3",
            "a93c5247776943cf849e3501f44cb0c2",
            "8faf61dcc0cd440bb96601eec2682124"
          ]
        },
        "id": "NJex4mm97JSr",
        "outputId": "2bc8e3a4-3b02-4442-bf0e-8cbb4468f2f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Found cached dataset thaisum (/root/.cache/huggingface/datasets/thaisum/thaisum/1.0.0/347b33c852af4d796e1224e00e15142d626608a9fa3e07ad6d19dfd8fcae5423)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "877284d0f1cc48c88b62187ebd4913a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = datasets.load_dataset(\"thaisum\")\n",
        "# dataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
        "# print(next(iter(ds))[\"code\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SIm7BgILALy",
        "outputId": "2ae2d227-1cc9-42e5-9f93-6cda158f8ce6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "datasets.dataset_dict.DatasetDict"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSgay9t9GDFC"
      },
      "source": [
        "## 2. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uyLR7aZGDFD"
      },
      "source": [
        "### Tokenizing\n",
        "\n",
        "Simply tokenize the given text to tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFIky2HVEs6m",
        "outputId": "3ee9036c-895e-4638-9d82-6e4d0d59044e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ผม', 'ไป', 'กินข้าว']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_tokenize(text = \"ผมไปกินข้าว\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2wQ4LhwGDFD",
        "outputId": "db7fea5d-1f57-4b7f-d0d3-460db48504fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/thaisum/thaisum/1.0.0/347b33c852af4d796e1224e00e15142d626608a9fa3e07ad6d19dfd8fcae5423/cache-6234a21ebe2b3394.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/thaisum/thaisum/1.0.0/347b33c852af4d796e1224e00e15142d626608a9fa3e07ad6d19dfd8fcae5423/cache-4d77ba36ca17542d.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/thaisum/thaisum/1.0.0/347b33c852af4d796e1224e00e15142d626608a9fa3e07ad6d19dfd8fcae5423/cache-53c267ce69a32e9b.arrow\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['อินดี้', 'จาก', 'อุดร', ' ', 'โอ๊ค', '-', 'สุ', 'พัฒน', '์', 'กิจ', ' ', 'ถวิล', 'การ', ' ', 'เล่น', 'กีตาร์', 'และ', 'ร้อง', 'นำ', ' ', 'ดูโอ', 'กับ', ' ', 'ทัด', '-', 'ณัตฐ', 'พงษ์', ' ', 'สุทธิ', 'วงศ์', 'กร', ' ', 'ตี', 'กลอง', ' ', 'พวกเขา', 'ทั้งสอง', 'ทำ', 'เพลง', 'มีเสน่ห์', ' ', 'การเขียน', 'เนื้อเพลง', 'แบบ', 'ย้ำคำ', 'ซ้ำๆ', ' ', 'วน', 'ไป', 'ให้', 'ติด', 'หู', 'แต่', 'ไม่', 'เข้าใจ', ' ', 'กับ', 'กีตาร์', 'ที่', 'มี', 'ความ', 'พริ้ม', 'เป็น', ' ', 'Dream', ' ', 'Pop', ' ', 'ล่องลอย', ' ', 'บาง', 'เพลง', 'เป็น', ' ', 'Shoegaze', ' ', 'ได้', 'เฉย', 'เลย', ' ']\n"
          ]
        }
      ],
      "source": [
        "tokenizer = word_tokenize\n",
        "\n",
        "#function to tokenize\n",
        "tokenize_data = lambda example, tokenizer: {'tokens': word_tokenize(example['summary'])}  \n",
        "\n",
        "#map the function to each example\n",
        "tokenized_dataset = ds.map(tokenize_data, remove_columns=['summary'], fn_kwargs={'tokenizer': tokenizer})\n",
        "print(tokenized_dataset['train'][50000]['tokens'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jw4ZtyhSjAt",
        "outputId": "8a0386de-801e-4e38-b5c0-6199ba35350c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ครอบครัว', 'หนุ่ม', 'สุราษฎร์ฯ', ' ', 'เข้าพบ', 'ผู้ใหญ่บ้าน', 'หมู่', ' ', '10', ' ', 'เมือง', 'คอน', ' ', 'แสดงตัว', 'เป็นเจ้าของ', ' ', 'เจ้า', ' ', 'บิ๊ก', 'บู', ' ', 'สุนัข', 'ตกรถ', ' ', 'เพศ', 'ผู้', ' ', 'อารมณ์ดี', ' ', 'เผย', 'วินาที', 'แรก', 'ที่', 'เจอ', 'หน้า', 'กลับ', 'เฉย', ' ', 'แต่', 'พอ', 'ถูก', 'ลูบคลำ', 'และ', 'เอา', 'ผล', 'ปาล์ม', 'ให้', 'กิน', 'กลับ', 'แสดง', 'ความดีใจ']\n"
          ]
        }
      ],
      "source": [
        "print(tokenized_dataset['train'][50]['tokens'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjLJ4vw3Jj4g"
      },
      "source": [
        "test thailand language tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWzHiJrIFDjX",
        "outputId": "c97e3535-e8fe-4c5a-9d9e-862de3122751"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['title', 'body', 'type', 'tags', 'url', 'tokens'],\n",
              "        num_rows: 358868\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['title', 'body', 'type', 'tags', 'url', 'tokens'],\n",
              "        num_rows: 11000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['title', 'body', 'type', 'tags', 'url', 'tokens'],\n",
              "        num_rows: 11000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kceerS9ZGDFE"
      },
      "source": [
        "### Numericalizing\n",
        "\n",
        "We will tell torchtext to add any word that has occurred at least three times in the dataset to the vocabulary because otherwise it would be too big.  Also we shall make sure to add `unk` and `eos`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obP-QslNGDFE",
        "outputId": "3b18ff72-bdb1-417d-ce00-81c64f98af28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44161\n",
            "['<unk>', '<eos>', ' ', 'ที่', 'ใน', 'และ', 'ไม่', 'ของ', 'มี', 'การ']\n"
          ]
        }
      ],
      "source": [
        "## numericalizing\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], \n",
        "min_freq=3) \n",
        "vocab.insert_token('<unk>', 0)           \n",
        "vocab.insert_token('<eos>', 1)            \n",
        "vocab.set_default_index(vocab['<unk>'])   \n",
        "print(len(vocab))                         \n",
        "print(vocab.get_itos()[:10])       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9Ie9qEPGDFF"
      },
      "source": [
        "## 3. Prepare the batch loader\n",
        "\n",
        "### Prepare data\n",
        "\n",
        "Given \"Chaky loves eating at AIT\", and \"I really love deep learning\", and given batch size = 3, we will get three batches of data \"Chaky loves eating at\", \"AIT `<eos>` I really\", \"love deep learning `<eos>`\".  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "POWm3c8sGDFF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_data(dataset, vocab, batch_size):\n",
        "    data = []                                                   \n",
        "    for example in dataset:\n",
        "        if example['tokens']:         \n",
        "            #appends eos so we know it ends....so model learn how to end...                             \n",
        "            tokens = example['tokens'].append('<eos>')   \n",
        "            #numericalize          \n",
        "            tokens = [vocab[token] for token in example['tokens']] \n",
        "            data.extend(tokens)                                    \n",
        "    data = torch.LongTensor(data)                                 \n",
        "    num_batches = data.shape[0] // batch_size \n",
        "    data = data[:num_batches * batch_size]                       \n",
        "    data = data.view(batch_size, num_batches)          \n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESx-bOwqTc6p",
        "outputId": "5590ce40-05cc-4ef8-d8db-0c843e3ef167"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "358868"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenized_dataset['train'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3IfVSkgpGDFF"
      },
      "outputs": [],
      "source": [
        "batch_size = 200\n",
        "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
        "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
        "test_data  = get_data(tokenized_dataset['test'], vocab, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syXLiiVmLQWH",
        "outputId": "c7dbcc46-1331-48a2-c504-d3ce28cc4ae1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUcOIUK1GDFG"
      },
      "source": [
        "## 4. Modeling "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FHmNzdscGDFG",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class LSTMLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
        "                \n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hid_dim = hid_dim\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, \n",
        "                    dropout=dropout_rate, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
        "        \n",
        "        self.init_weights()\n",
        "        \n",
        "    def init_weights(self):\n",
        "        init_range_emb = 0.1\n",
        "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
        "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
        "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
        "        self.fc.bias.data.zero_()\n",
        "        for i in range(self.num_layers):\n",
        "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
        "                    self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
        "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim, \n",
        "                    self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
        "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
        "        return hidden, cell\n",
        "    \n",
        "    def detach_hidden(self, hidden):\n",
        "        hidden, cell = hidden\n",
        "        hidden = hidden.detach()\n",
        "        cell = cell.detach()\n",
        "        return hidden, cell\n",
        "\n",
        "    def forward(self, src, hidden):\n",
        "        #src: [batch size, seq len]\n",
        "        embedding = self.dropout(self.embedding(src))\n",
        "        #embedding: [batch size, seq len, emb_dim]\n",
        "        output, hidden = self.lstm(embedding, hidden)      \n",
        "        #output: [batch size, seq len, hid_dim]\n",
        "        #hidden = h, c = [num_layers * direction, seq len, hid_dim)\n",
        "        output = self.dropout(output) \n",
        "        prediction = self.fc(output)\n",
        "        #prediction: [batch size, seq_len, vocab size]\n",
        "        return prediction, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2OmvpmPGDFH"
      },
      "source": [
        "## 5. Training \n",
        "\n",
        "Follows very basic procedure.  One note is that some of the sequences that will be fed to the model may involve parts from different sequences in the original dataset or be a subset of one (depending on the decoding length). For this reason we will reset the hidden state every epoch, this is like assuming that the next batch of sequences is probably always a follow up on the previous in the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "from torchtext.vocab import Vocab\n",
        "\n",
        "# define file path for loading\n",
        "file_path = 'vocab.pkl'\n",
        "\n",
        "# load the Vocab object\n",
        "with open(file_path, 'rb') as f:\n",
        "    vocab = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3DZKAmqKGDFH"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "emb_dim = 1024                # 400 in the paper\n",
        "hid_dim = 1024                # 1150 in the paper\n",
        "num_layers = 2                # 3 in the paper\n",
        "dropout_rate = 0.65              \n",
        "lr = 1e-3                     \n",
        "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxBiHOmsGDFI",
        "outputId": "c4248b89-fd46-4bf3-ab38-853428bac5b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 107,279,489 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {num_params:,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "eKH2HFQmGDFI"
      },
      "outputs": [],
      "source": [
        "def get_batch(data, seq_len, idx):\n",
        "    src    = data[:, idx:idx+seq_len]                   \n",
        "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
        "    return src, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Xye8E8OuGDFI"
      },
      "outputs": [],
      "source": [
        "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    # drop all batches that are not a multiple of seq_len\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    \n",
        "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
        "        optimizer.zero_grad()\n",
        "        hidden = model.detach_hidden(hidden)\n",
        "\n",
        "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
        "        src, target = src.to(device), target.to(device)\n",
        "        batch_size = src.shape[0]\n",
        "        prediction, hidden = model(src, hidden)               \n",
        "\n",
        "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
        "        target = target.reshape(-1)\n",
        "        loss = criterion(prediction, target)\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZtHkAT3YGDFJ"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    model.eval()\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in range(0, num_batches - 1, seq_len):\n",
        "            hidden = model.detach_hidden(hidden)\n",
        "            src, target = get_batch(data, seq_len, idx)\n",
        "            src, target = src.to(device), target.to(device)\n",
        "            batch_size= src.shape[0]\n",
        "\n",
        "            prediction, hidden = model(src, hidden)\n",
        "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
        "            target = target.reshape(-1)\n",
        "\n",
        "            loss = criterion(prediction, target)\n",
        "            epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "Lp4vlqOYGDFJ",
        "outputId": "e306bd36-dc99-4522-cdbe-5c3f39bedbbe"
      },
      "outputs": [],
      "source": [
        "n_epochs = 5\n",
        "seq_len  = 50\n",
        "clip    = 0.25\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_data, optimizer, criterion, \n",
        "                batch_size, seq_len, clip, device)\n",
        "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
        "                seq_len, device)\n",
        "\n",
        "    lr_scheduler.step(valid_loss)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
        "\n",
        "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVNpJgQBGDFJ"
      },
      "source": [
        "## 6. Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sb0HP3iGDFJ",
        "outputId": "d37ca13b-beeb-45c8-8eaf-7a85713ff803"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
        "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
        "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neEUXMv4GDFK"
      },
      "source": [
        "## 7. Real-world inference\n",
        "\n",
        "Here we take the prompt, tokenize, encode and feed it into the model to get the predictions.  We then apply softmax while specifying that we want the output due to the last word in the sequence which represents the prediction for the next word.  We divide the logits by a temperature value to alter the model’s confidence by adjusting the softmax probability distribution.\n",
        "\n",
        "Once we have the Softmax distribution, we randomly sample it to make our prediction on the next word. If we get <unk> then we give that another try.  Once we get <eos> we stop predicting.\n",
        "    \n",
        "We decode the prediction back to strings last lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_2YFS5i3GDFK"
      },
      "outputs": [],
      "source": [
        "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "    model.eval()\n",
        "    tokens = tokenizer(prompt)\n",
        "    indices = [vocab[t] for t in tokens]\n",
        "    batch_size = 1\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(max_seq_len):\n",
        "            src = torch.LongTensor([indices]).to(device)\n",
        "            prediction, hidden = model(src, hidden)\n",
        "            \n",
        "            #prediction: [batch size, seq len, vocab size]\n",
        "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
        "            \n",
        "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
        "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
        "            \n",
        "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
        "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
        "                break\n",
        "\n",
        "            indices.append(prediction) #autoregressive, thus output becomes input\n",
        "\n",
        "    itos = vocab.get_itos()\n",
        "    tokens = [itos[i] for i in indices]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-52Y4oesGDFK",
        "outputId": "5b86dd7f-73bb-478a-fc9f-01f071d2566f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5\n",
            "ควยช้างป่า จ.เลย ถูกขวิดเสียชีวิต \n",
            "0.7\n",
            "ควยช้างป่า จ.เลย ถูกพัดทำร้ายลงมา มีรอยช้ำ \n",
            "0.75\n",
            "ควยช้างป่า จ.เลย ถูกพัดทำร้ายลงมา มีรอยช้ำที่แขน \n",
            "0.8\n",
            "ควยช้างป่า จ.เลย ถูกพัดทำร้ายลงมาอย่างมีเงื่อนงำและความรุนแรงในช่วงหน้าแล้ง\n",
            "1.0\n",
            "ควยหัวใจสลาย ขาดหมอ ส่วนเหยื่อคนซ้อนไม่มีใครและความเข้าเวรไทย ทำเอาคนฟังต่างฝ่ายต่างบอกบ้านเราในเรื่องราวที่อยากให้ลูกหนีข่าวได้ คาดเป็นคนที่เล่า มีแม่ม่ายสุนัขช่วยบริจาคผู้พิการหลายแสน เนื่องจากมีคนแจ้งว่าเป็นของขวัญ\n"
          ]
        }
      ],
      "source": [
        "prompt = 'ควย'\n",
        "max_seq_len = 60\n",
        "seed = 0\n",
        "\n",
        "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
        "for temperature in temperatures:\n",
        "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
        "                          vocab, device, seed)\n",
        "    \n",
        "    print(str(temperature)+'\\n'+''.join(generation))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "03c0b7698bc8ddcbc4ac2f5708958d09d9f2028b88f43ee7158b288fbdf68079"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2ddf5268bb214f7a9bae9bec1781e547": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "415c30dc5fcf4790bca17e9124fa6bd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a93c5247776943cf849e3501f44cb0c2",
            "placeholder": "​",
            "style": "IPY_MODEL_8faf61dcc0cd440bb96601eec2682124",
            "value": " 3/3 [00:02&lt;00:00,  2.59s/it]"
          }
        },
        "877284d0f1cc48c88b62187ebd4913a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf6d2b43256a47f68490e2b91b4751d1",
              "IPY_MODEL_d6db6d415a9b41a6989b3558767f75bf",
              "IPY_MODEL_415c30dc5fcf4790bca17e9124fa6bd1"
            ],
            "layout": "IPY_MODEL_2ddf5268bb214f7a9bae9bec1781e547"
          }
        },
        "8faf61dcc0cd440bb96601eec2682124": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5aac3a0b556497694bc5f4e8a34514a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a93c5247776943cf849e3501f44cb0c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf6d2b43256a47f68490e2b91b4751d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6d6dc065dac4a9db5ed37f58506bc62",
            "placeholder": "​",
            "style": "IPY_MODEL_f4417c7459e74c3a88cd20a7dbd85fd3",
            "value": "100%"
          }
        },
        "d6db6d415a9b41a6989b3558767f75bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5aac3a0b556497694bc5f4e8a34514a",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6fd3468f5854e61a956af740efddbc3",
            "value": 3
          }
        },
        "f4417c7459e74c3a88cd20a7dbd85fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6d6dc065dac4a9db5ed37f58506bc62": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6fd3468f5854e61a956af740efddbc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
