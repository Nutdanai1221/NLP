{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing Homework\n",
    "\n",
    "## Nutdanai Sritunya st123055 MMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm  #gimmick for progressbar when you train\n",
    "import pickle #saving and loading models\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parsing function\n",
    "\n",
    "We gonna start with a class `Parsing`, representing a parser for each sentence.  For each sentence, we need the `stack`, `buffer`, and the `dependencies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basically, it takes the current state of the buffer, stack, dependencies\n",
    "# #tell us how SHIFT, LA, RA changes these three objects\n",
    "\n",
    "class Parsing(object):\n",
    "    \n",
    "    #init stack, buffer, dep\n",
    "    def __init__(self, sentence):  \n",
    "        self.sentence = sentence     #['The', 'cat', 'sat]  #conll format which is already in the tokenized form\n",
    "        self.stack    = ['ROOT']\n",
    "        self.buffer   = sentence[:]  #in the beginning, everything is inside the buffer\n",
    "        self.dep      = []           #maintains a list of tuples of dep\n",
    "    \n",
    "    #parse function that tells me how shift, la, ra changes these three objects\n",
    "    def parse_step(self, transition):     #transition could be either S, LA, RA\n",
    "        if transition == 'S':\n",
    "            #get the top guy in the buffer and put in stack\n",
    "            head = self.buffer.pop(0)\n",
    "            self.stack.append(head)\n",
    "        elif transition == 'LA':  #stack = [ROOT, He, has] ==> append to dep (has, he) and then He is gone from the stack [ROOT, has]\n",
    "            dependent = self.stack.pop(-2)  #He\n",
    "            self.dep.append((self.stack[-1], dependent))  #(has, he)\n",
    "        elif transition == 'RA':\n",
    "            #can you guys try to this???\n",
    "            dependent = self.stack.pop()  #stack = [ROOT, has, control] ==> dep (has, control), control will be gone fromt he stack [ROOT, has]\n",
    "            self.dep.append((self.stack[-1], dependent))\n",
    "        else:\n",
    "            print(f\"Bad transition: {transition}\")\n",
    "    \n",
    "    #given some series of transition, it gonna for-loop the parse function\n",
    "    def parse(self, transitions):\n",
    "        for t in transitions:\n",
    "            self.parse_step(t)\n",
    "        return self.dep\n",
    "    \n",
    "    #check whether things are finished - no need to do anymore functions....\n",
    "    def is_completed(self):\n",
    "        return (len(self.buffer) == 0) and (len(self.stack) == 1)  #so buffer is empty and ROOT is the only guy in stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch parsing\n",
    "\n",
    "We gonna create a minibatch loader that loads a bunch of sentences, and perform parse accordingly.  For now, we will assume a very dump model to predict the transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_parse(sentences, model, batch_size):\n",
    "    dep = []  #all the resulting dep\n",
    "    \n",
    "    #init Parsing instance for each sentence in the batch\n",
    "    partial_parses = [Parsing(sentence) for sentence in sentences]  #in tokenized form\n",
    "    #Parsing(['The', 'cat', 'sat']), Parsing(['Chaky', 'is', 'mad'])\n",
    "    \n",
    "    unfinished_parses = partial_parses[:]\n",
    "    \n",
    "    #while we still have sentence\n",
    "    while unfinished_parses:  #if there are still a Parsing object\n",
    "    \n",
    "        #take a certain batch of sentence\n",
    "        minibatch = unfinished_parses[:batch_size] #number of Parsing object\n",
    "        \n",
    "        #create a dummy model to tell us what's the next transition for each sentence\n",
    "        transitions = model.predict(minibatch) \n",
    "        #transitions = [S, S, .....]\n",
    "        #minibatch   = [Parsing(sentence1), Parsing(sentence2)]\n",
    "        \n",
    "                \n",
    "        # for transition predicted this dummy model\n",
    "        for transition, partial_parse in zip(transitions, minibatch):\n",
    "            #parse step\n",
    "            #transition: S\n",
    "            #partial_parse: Parsing(sentence)\n",
    "            partial_parse.parse_step(transition)\n",
    "            \n",
    "        #remove any sentence is finish\n",
    "        unfinished_parses[:] = [p for p in unfinished_parses if not p.is_completed()]\n",
    "    \n",
    "    dep = [parse.dep for parse in partial_parses]\n",
    "    \n",
    "    return dep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll(filename):\n",
    "    \n",
    "    examples = []\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        i = 0\n",
    "        word, pos, head, dep = [], [], [], []\n",
    "        for line in f.readlines():\n",
    "            i = i+1\n",
    "            wa = line.strip().split('\\t')  #['1', 'In', '_', 'ADP', 'IN', '_', '5', 'case', '_', '_']\n",
    "            #In <--------  5th guy\n",
    "            #     case\n",
    "            \n",
    "            if len(wa) == 10:  #if all the columns are there\n",
    "                word.append(wa[1].lower())\n",
    "                pos.append(wa[4])\n",
    "                head.append(int(wa[6]))\n",
    "                dep.append(wa[7])\n",
    "            \n",
    "            #the row is not exactly 10, it means new sentence\n",
    "            elif len(word) > 0:  #if there is somethign inside the word\n",
    "                examples.append({'word': word, 'pos': pos, 'head': head, 'dep': dep})  #in the sentence level\n",
    "                word, pos, head, dep = [], [], [], [] #clear word, pos, head, dep\n",
    "        \n",
    "        if len(word) > 0:  #if there is somethign inside the word\n",
    "            examples.append({'word': word, 'pos': pos, 'head': head, 'dep': dep})  #in the sentence level\n",
    "\n",
    "    return examples                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print(\"1. Loading data\")\n",
    "    train_set = read_conll(\"data/train.conll\")\n",
    "    dev_set   = read_conll(\"data/dev.conll\")\n",
    "    test_set   = read_conll(\"data/test.conll\")\n",
    "    \n",
    "    #make my dataset smaller because my mac cannot handle it\n",
    "    train_set = train_set[:1000]\n",
    "    dev_set   = dev_set[:500]\n",
    "    test_set  = test_set[:500]\n",
    "    \n",
    "    return train_set, dev_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the load function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading data\n"
     ]
    }
   ],
   "source": [
    "train_set, dev_set, test_set = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 500, 500)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(dev_set), len(test_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand, we can draw these in a dependency tree, with the help of spaCy.  **Note** that spaCy do not draw the ROOT for us, but imagine the head of \"plays\" is ROOT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"677a30e901064beba8c95164b6884d62-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Ms.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Haag</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">plays</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Elianti</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PUNCT</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-677a30e901064beba8c95164b6884d62-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-677a30e901064beba8c95164b6884d62-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-677a30e901064beba8c95164b6884d62-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-677a30e901064beba8c95164b6884d62-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-677a30e901064beba8c95164b6884d62-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-677a30e901064beba8c95164b6884d62-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-677a30e901064beba8c95164b6884d62-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-677a30e901064beba8c95164b6884d62-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#we eventually gonna make the dependency\n",
    "#so maybe we can cheat a little bit, and see the answer\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy #displacy is for visualization\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Ms. Haag plays Elianti .\")\n",
    "options = {\"collapse_punct\": False}\n",
    "\n",
    "displacy.render(doc, options = options, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gonna create a parser that gonna help us:\n",
    "- create a `tok2id` dictionary in the `__init__` function\n",
    "- numercalize `numercalize` the words, dependencies, and pos tags\n",
    "- create training data, `create_instances` by leveraging the ground truth of the dependencies\n",
    "- finally the `parse` function\n",
    "\n",
    "This feature vector consists of a list of tokens. They can be represented as a list of integers $\\mathbf{w} = [w_1, w_2, \\cdots, w_m]$ where $m$ is the number of features and each $0 \\leq w_i \\leq |V|$ is the index of a token in the vocabulary ($|V|$ is the vocabulary size).  Then our network looks up an embedding for each word and concatenates them into a single input vector:\n",
    "\n",
    "$$\\mathbf{x} = [\\mathbf{E}_{w_1}, \\cdots, \\mathbf{E}_{w_m}] \\in \\mathbb{R}^{dm}$$\n",
    "\n",
    "where $\\mathbf{E} \\in \\mathbb{R}^{|V| \\times d}$ is an embedding matrix with each row $\\mathbf{E}_w$ as the vector for a particular word $w$\n",
    "\n",
    "#### Features\n",
    "\n",
    "A total of 18 + 18 + 12 features were used in the paper.\n",
    "\n",
    "- Feature 1: 18 features\n",
    "  - (a). 6 - top 3 words on buffer, top 3 words on stack, \n",
    "  - (b). 6 - the first and second left most/rightmost children and the leftmost/rightmost of the **top word** (i.e., `stack[-1]`) on the stack,  - `(leftmost(0), rightmost(0), secondleftmost(0), secondrightmost(0), leftmost(leftmost(0)), rightmost(rightmost(0)))`\n",
    "  - (c). 6 - the first and second left most/rightmost children and the leftmost/rightmost of the **second top word** (i.e., `stack[-2]`) on the stack, - `(leftmost(0), rightmost(0), secondleftmost(0), secondrightmost(0), leftmost(leftmost(0)), rightmost(rightmost(0)))`\n",
    "- Feature 2: 18 pos - basically corresponding POS tags\n",
    "- Feature 3: 12 dep - corresponding ARC, excluding 6 words on the stack/buffer..\n",
    "\n",
    "**NOTE**: There is no 3a because each word itself does not have dependency\n",
    "\n",
    "**NOTE**: For brevity, the table skipped 2b, 2c\n",
    "\n",
    "**NOTE**: Here, in the dependency column, I used LABELED dependency.   But in my code, it is unlabeled, thus e.g., COMPOUND will not be needed.\n",
    "\n",
    "| Stack | Buffer | Feature 1a | Feature 1b | Feature 1c | Feature 2a | Feature 3b | Feature 3c | Dependency (ARC) | Transition |\n",
    "| :--   |  :--   | :--        | :--        | :--        | :--        | :--        | :--        | :--               | :--        |\n",
    "| [ROOT] | [Ms., Haag, plays, Elianti, .] | [NULL, NULL, ROOT, Ms., Haag, plays] | [NULL, NULL, NULL, NULL, NULL, NULL] | [NULL, NULL, NULL, NULL, NULL, NULL] | [P-NULL, P-NULL, P-ROOT, PROPN, PROPN, VERB] | [D-NULL, D-NULL, D-NULL, D-NULL, D-NULL, D-NULL]  | [D-NULL, D-NULL, D-NULL, D-NULL, D-NULL, D-NULL]| | Init |\n",
    "| [ROOT, Ms.] | [Haag, plays, Elianti, .] | [NULL, ROOT, Ms., Haag, plays, Elianti] | [NULL, NULL, NULL, NULL, NULL, NULL]  | [NULL, NULL, NULL, NULL, NULL, NULL] | [P-NULL, P-ROOT, PROPN, PROPN, VERB, PROPN] | [D-NULL, D-NULL, D-NULL, D-NULL, D-NULL, D-NULL]  | [D-NULL, D-NULL, D-NULL, D-NULL, D-NULL, D-NULL]| | SHIFT |\n",
    "| [ROOT, Ms., Haag] | [plays, Elianti, .] | [ROOT, Ms., Haag, plays, Elianti, .] | [NULL, NULL, NULL, NULL, NULL, NULL]  | [NULL, NULL, NULL, NULL, NULL, NULL] | [P-ROOT, PROPN, PROPN, VERB, PROPN, PUNCT] | [D-NULL, D-NULL, D-NULL, D-NULL, D-NULL, D-NULL]  | [D-NULL, D-NULL, D-NULL, D-NULL, D-NULL, D-NULL]|  | SHIFT |\n",
    "| [ROOT, Haag] | [plays, Elianti, .] | [NULL, ROOT, Haag, plays, Elianti, .] | [Ms., NULL, NULL, NULL, NULL, NULL, NULL] | [NULL, NULL, NULL, NULL, NULL, NULL] | [P-NULL, P-ROOT, PROPN, VERB, PROPN, PUNCT] | [COMPOUND, D-NULL, D-NULL, D-NULL, D-NULL, D-NULL] | [D-NULL, D-NULL, D-NULL, D-NULL, D-NULL, D-NULL] | (Haag, Ms., COMPOUND) | LEFTARC |\n",
    "| [ROOT, Haag, plays] | [Elianti, .] | [ROOT, Haag, plays, Elianti, ., NULL] | [NULL, NULL, NULL, NULL, NULL, NULL] | [Ms., NULL, NULL, NULL, NULL, NULL] | [P-ROOT, PROPN, VERB, PROPN, PUNCT, P-NULL] | [D-NULL, D-NULL, D-NULL, D-NULL, D-NULL, D-NULL] | [COMPOUND, D-NULL, D-NULL, D-NULL, D-NULL, D-NULL] | (Haag, Ms., COMPOUND) | SHIFT |\n",
    "| [ROOT, plays] | [Elianti, .] | [NULL, ROOT, plays, Elianti, ., NULL] | [Haag, NULL, NULL, NULL, Ms., NULL] | [NULL, NULL, NULL, NULL, NULL, NULL] | [P-NULL, P-ROOT, VERB, PROPN, PUNCT, P-NULL] | [NSUBJ, D-NULL, D-NULL, D-NULL, COMPOUND, D-NULL] | [D-NULL, D-NULL, D-NULL, D-NULL, D-NULL, D-NULL] | (Haag, Ms., COMPOUND), (plays, Haag, NSUBJ) | LEFTARC |\n",
    "| [ROOT, plays, Elianti] | [.] | [ROOT, plays, Elianti, ., NULL, NULL] | [NULL, NULL, NULL, NULL, NULL, NULL] | [Haag, NULL, NULL, NULL, Ms., NULL] | [P-ROOT, VERB, PROPN, PUNCT, P-NULL, P-NULL] | [D-NULL, D-NULL, D-NULL, D-NULL, D-NULL, D-NULL] | [NSUBJ, D-NULL, D-NULL, D-NULL, COMPOUND, D-NULL] | (Haag, Ms., COMPOUND), (plays, Haag, NSUBJ) | SHIFT |\n",
    "| [ROOT, plays] | [.] | [NULL, ROOT, plays, ., NULL, NULL] | [Haag, Elianti, NULL, NULL, Ms., NULL] | [NULL, NULL, NULL, NULL, NULL, NULL] | [P-NULL, P-ROOT, VERB, PUNCT, P-NULL, P-NULL] | [NSUBJ, DOBJ, D-NULL, D-NULL, COMPOUND, D-NULL] | [D-NULL, D-NULL, D-NULL, D-NULL, D-NULL, D-NULL] | (Haag, Ms., COMPOUND), (plays, Haag, NSUBJ), (plays, Elianti, DOBJ) | RIGHTARC |\n",
    "| [ROOT, plays, .] | [] | [ROOT, plays, ., NULL, NULL, NULL] | [NULL, NULL, NULL, NULL, NULL, NULL] | [Haag, Elianti, NULL, NULL, Ms., NULL] | [P-NULL, P-ROOT, VERB, PUNCT, P-NULL, P-NULL] | [D-NULL, D-NULL, D-NULL, D-NULL, D-NULL, D-NULL] | [NSUBJ, DOBJ, D-NULL, D-NULL, COMPOUND, D-NULL] | (Haag, Ms., COMPOUND), (plays, Haag, NSUBJ), (plays, Elianti, DOBJ) | SHIFT |\n",
    "| [ROOT, plays] | [] | [NULL, ROOT, plays, NULL, NULL, NULL] | [Haag, Elianti, NULL, plays, Ms., NULL] | [Haag, Elianti, NULL, NULL, Ms., NULL] | [P-NULL, P-ROOT, VERB, P-NULL, P-NULL, P-NULL] | [NSUBJ, PUNCT, D-NULL, DOBJ, COMPOUND, D-NULL] | [D-NULL, D-NULL, D-NULL, D-NULL, D-NULL, D-NULL] | (Haag, Ms., COMPOUND), (plays, Haag, NSUBJ), (plays, Elianti, DOBJ), (plays, ., PUNCT) | RIGHT-ARC |\n",
    "| [ROOT] | [] |  | |  |  |  |  | (Haag, Ms., COMPOUND), (plays, Haag, NSUBJ), (plays, Elianti, DOBJ), (plays, ., PUNCT), (ROOT, plays, ROOT) | RIGHT-ARC |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_PREFIX = '<p>:' #indicating pos tags\n",
    "D_PREFIX = '<d>:' #indicating dependency tags\n",
    "UNK      = '<UNK>'\n",
    "NULL     = '<NULL>'\n",
    "ROOT     = '<ROOT>'\n",
    "\n",
    "class Parser(object):\n",
    "\n",
    "    def __init__(self, dataset, dep_logi=True, pos_logi=True):\n",
    "        \n",
    "        #set the root dep\n",
    "        self.root_dep = 'root'\n",
    "        self.dep_logi = dep_logi\n",
    "        self.pos_logi = pos_logi\n",
    "                \n",
    "        #get all the dep of the dataset as list, e.g., ['root', 'acl', 'nmod', 'nmod:npmod']\n",
    "        all_dep = [self.root_dep] + list(set([w for ex in dataset\n",
    "                                               for w in ex['dep']\n",
    "                                               if w != self.root_dep]))\n",
    "        \n",
    "        #1. put dep into tok2id lookup table, with D_PREFIX so we know it is dependency\n",
    "        #{'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'D_PREFIX:<NULL>': 30}\n",
    "        tok2id = {D_PREFIX + l: i for (i, l) in enumerate(all_dep)}\n",
    "        tok2id[D_PREFIX + NULL] = self.D_NULL = len(tok2id)\n",
    "        \n",
    "        #we are using \"unlabeled\" where we do not label with the dependency\n",
    "        #thus the number of dependency relation is 1\n",
    "        trans = ['L', 'R', 'S']\n",
    "        self.n_deprel = 1   #because we are not predicting the relations, we are only predicting S, L, R\n",
    "        # self.n_features = 0\n",
    "        #create a simple lookup table mapping action and id\n",
    "        #e.g., tran2id: {'L': 0, 'R': 1, 'S': 2}\n",
    "        #e.g., id2tran: {0: 'L', 1: 'R', 2: 'S'}\n",
    "        self.n_trans = len(trans)\n",
    "        self.tran2id = {t: i for (i, t) in enumerate(trans)}  #use for easy coding\n",
    "        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n",
    "        print(self.dep_logi)\n",
    "        print(self.pos_logi)\n",
    "        #2. put pos tags into tok2id lookup table, with P_PREFIX so we know it is pos\n",
    "        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[P_PREFIX + UNK]  = self.P_UNK  = len(tok2id)  #also remember the pos tags of unknown\n",
    "        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
    "        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
    "        \n",
    "        #now tok2id:  {'P_PREFIX:root': 0, 'P_PREFIX:acl': 1, ..., 'P_PREFIX:JJR': 62, 'P_PREFIX:<UNK>': 63, 'P_PREFIX:<NULL>': 64, 'P_PREFIX:<ROOT>': 65}\n",
    "        \n",
    "        #3. put word into tok2id lookup table\n",
    "        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[UNK]  = self.UNK = len(tok2id)\n",
    "        tok2id[NULL] = self.NULL = len(tok2id)\n",
    "        tok2id[ROOT] = self.ROOT = len(tok2id)\n",
    "        \n",
    "        #now tok2id: {'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'memory': 340, 'mr.': 341, '<UNK>': 342, '<NULL>': 343, '<ROOT>': 344}\n",
    "        # self.n_features = 0\n",
    "        #create id2tok\n",
    "        self.tok2id = tok2id\n",
    "        self.id2tok = {v: k for (k, v) in tok2id.items()}\n",
    "        if self.dep_logi == True and self.pos_logi == True :\n",
    "            self.n_features = 18 + 18 + 12\n",
    "        elif self.dep_logi == True and self.pos_logi == False :\n",
    "            self.n_features = 18 + 12\n",
    "        elif self.dep_logi == False and self.pos_logi == True :\n",
    "            self.n_features = 18 + 18\n",
    "        else :\n",
    "            pass\n",
    "        self.n_tokens = len(tok2id)\n",
    "        print(self.n_features)\n",
    "        \n",
    "    #utility function, in case we want to convert token to id\n",
    "    #function to turn train set with words to train set with id instead using tok2id\n",
    "    def numericalize(self, examples):\n",
    "        numer_examples = []\n",
    "        for ex in examples:\n",
    "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
    "                                  else self.UNK for w in ex['word']]\n",
    "            pos  = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
    "                                   else self.P_UNK for w in ex['pos']]\n",
    "            head = [-1] + ex['head']\n",
    "            dep  = [-1] + [self.tok2id[D_PREFIX + w] if D_PREFIX + w in self.tok2id\n",
    "                            else -1 for w in ex['dep']]\n",
    "            numer_examples.append({'word': word, 'pos': pos,\n",
    "                                 'head': head, 'dep': dep})\n",
    "        return numer_examples\n",
    "    \n",
    "    #function to extract features to form a feature embedding matrix\n",
    "    def extract_features(self, stack, buf, arcs, ex):\n",
    "             \n",
    "        #ex['word']:  [55, 32, 33, 34, 35, 30], i.e., ['root', 'ms.', 'haag', 'plays', 'elianti', '.']\n",
    "        #ex['pos']:   [29, 14, 14, 16, 14, 17], i.e., ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
    "        #ex['head']:  [-1, 2, 3, 0, 3, 3]  or ['root', 'compound', 'nsubj', 'root', 'dobj', 'punct']}\n",
    "        #ex['dep']:   [-1, 1, 2, 0, 6, 12] or ['compound', 'nsubj', 'root', 'dobj', 'punct']\n",
    "\n",
    "        #stack     :  [0]\n",
    "        #buffer    :  [1, 2, 3, 4, 5]\n",
    "        \n",
    "        if stack[0] == \"ROOT\":\n",
    "            stack[0] = 0  #start the stack with [ROOT]\n",
    "            \n",
    "        p_features = [] #pos features (2a, 2b, 2c) - 18\n",
    "        d_features = [] #dep features (3b, 3c) - 12\n",
    "        \n",
    "        #last 3 things on the stack as features\n",
    "        #if the stack is less than 3, then we simply append NULL from the left\n",
    "        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n",
    "        \n",
    "        # next 3 things on the buffer as features\n",
    "        #if the buffer is less than 3, simply append NULL\n",
    "        #the reason why NULL is appended on end because buffer is read left to right\n",
    "        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n",
    "        \n",
    "        #corresponding pos tags\n",
    "        p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n",
    "        p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n",
    "        \n",
    "        #get leftmost children based on the dependency arcs\n",
    "        def get_lc(k):\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n",
    "\n",
    "        #get right most children based on the dependency arcs\n",
    "        def get_rc(k):\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n",
    "                          reverse=True)\n",
    "        \n",
    "        #get the leftmost and rightmost children of the top two words, thus we loop 2 times\n",
    "        for i in range(2):\n",
    "            if i < len(stack):\n",
    "                k = stack[-i-1] #-1, -2 last two in the stack\n",
    "                \n",
    "                #the first and second lefmost/rightmost children of the top two words (i=1, 2) on the stack\n",
    "                lc = get_lc(k)  \n",
    "                rc = get_rc(k)\n",
    "                \n",
    "                #the leftmost of leftmost/rightmost of rightmost children of the top two words on the stack:\n",
    "                llc = get_lc(lc[0]) if len(lc) > 0 else []\n",
    "                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n",
    "\n",
    "                #(leftmost of first word on stack, rightmost of first word, \n",
    "                # leftmost of the second word on stack, rightmost of second, \n",
    "                # leftmost of leftmost, rightmost of rightmost\n",
    "                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n",
    "\n",
    "                #corresponding pos\n",
    "                p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n",
    "            \n",
    "                #corresponding dep\n",
    "                d_features.append(ex['dep'][lc[0]] if len(lc) > 0 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][rc[0]] if len(rc) > 0 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][lc[1]] if len(lc) > 1 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][rc[1]] if len(rc) > 1 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][llc[0]] if len(llc) > 0 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][rrc[0]] if len(rrc) > 0 else self.D_NULL)\n",
    "                \n",
    "            else:\n",
    "                #attach NULL when they don't exist\n",
    "                features += [self.NULL] * 6\n",
    "                p_features += [self.P_NULL] * 6\n",
    "                d_features += [self.D_NULL] * 6\n",
    "        if self.dep_logi == True and self.pos_logi == True :\n",
    "            features += p_features + d_features\n",
    "        elif self.dep_logi == True and self.pos_logi == False :\n",
    "            features += d_features\n",
    "        elif self.dep_logi == False and self.pos_logi == True :\n",
    "            features += p_features\n",
    "        else :\n",
    "            pass         \n",
    "        \n",
    "        assert len(features) == self.n_features\n",
    "        \n",
    "        return features\n",
    "\n",
    "    #generate training examples\n",
    "    #from the training sentences and their gold parse trees \n",
    "    def create_instances(self, examples):  #examples = word, pos, head, dep\n",
    "        all_instances = []\n",
    "        \n",
    "        for i, ex in enumerate(examples):\n",
    "            #Ms. Haag plays Elianti .\n",
    "            #e.g., ex['word]: [344, 163, 99, 164, 165, 68]\n",
    "            #here 344 stands for ROOT\n",
    "            #Chaky - I cheated and take a look\n",
    "            n_words = len(ex['word']) - 1  #excluding the root\n",
    "            \n",
    "            #arcs = {(head, tail, dependency label)}\n",
    "            stack = [0]\n",
    "            buf = [i + 1 for i in range(n_words)]  #[1, 2, 3, 4, 5]\n",
    "            arcs = []\n",
    "            instances = []\n",
    "            \n",
    "            #because that's the maximum number of shift, leftarcs, rightarcs you can have\n",
    "            #this will determine the sample size of each training example\n",
    "            #if given five words, we will get a sample of (10, 48) where 10 comes from 5 * 2, and 48 is n_features\n",
    "            #but this for loop can be break if there is nothing left....\n",
    "            for i in range(n_words * 2):  #maximum times you can do either S, L, R\n",
    "                \n",
    "                #get the gold transition based on the parse trees\n",
    "                #gold_t can be either shift(2), leftarc(0), or rightarc(1)\n",
    "                gold_t = self.get_oracle(stack, buf, ex)\n",
    "                \n",
    "                #if gold_t is None, no need to extract features.....\n",
    "                if gold_t is None:\n",
    "                    break\n",
    "                \n",
    "                #make sure when the model predicts, we inform the current state of stack and buffer, so\n",
    "                #the model is not allowed to make any illegal action, e.g., buffer is empty but trying to pop\n",
    "                legal_labels = self.legal_labels(stack, buf)                \n",
    "                assert legal_labels[gold_t] == 1\n",
    "                \n",
    "                #extract all the 48 features \n",
    "                features = self.extract_features(stack, buf, arcs, ex)\n",
    "                instances.append((features, legal_labels, gold_t))\n",
    "                \n",
    "                #shift \n",
    "                if gold_t == 2:\n",
    "                    stack.append(buf[0])\n",
    "                    buf = buf[1:]\n",
    "                #left arc \n",
    "                elif gold_t == 0:\n",
    "                    arcs.append((stack[-1], stack[-2], gold_t))\n",
    "                    stack = stack[:-2] + [stack[-1]]\n",
    "                #right arc\n",
    "                else:\n",
    "                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n",
    "                    stack = stack[:-1]\n",
    "                    \n",
    "            else:\n",
    "                all_instances += instances\n",
    "\n",
    "        return all_instances\n",
    "    \n",
    "    #provide an one hot encoding of the labels\n",
    "    def legal_labels(self, stack, buf):\n",
    "        labels =  ([1] if len(stack) > 2  else [0]) * self.n_deprel  #left arc but you cannot do ROOT <--- He\n",
    "        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel  #right arc because ROOT --> He\n",
    "        labels += [1] if len(buf) > 0 else [0]  #shift\n",
    "        return labels\n",
    "    \n",
    "    #a simple function to check punctuation POS tags\n",
    "    def punct(self, pos):\n",
    "        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n",
    "    \n",
    "    #decide whether to shift, leftarc, or rightarc, based on gold parse trees\n",
    "    #this is needed to create training examples which contain samples and ground truth\n",
    "    def get_oracle(self, stack, buf, ex):\n",
    "        \n",
    "        #leave if the stack is only 1, thus nothing to predict....\n",
    "        if len(stack) < 2:\n",
    "            return self.n_trans - 1\n",
    "        \n",
    "        #predict based on the last two words on the stack\n",
    "        #stack: [ROOT, he, has]\n",
    "        i0 = stack[-1] #has\n",
    "        i1 = stack[-2] #he\n",
    "        \n",
    "        #get the head and dependency\n",
    "        h0 = ex['head'][i0]\n",
    "        h1 = ex['head'][i1]\n",
    "        d0 = ex['dep'][i0]\n",
    "        d1 = ex['dep'][i1]\n",
    "        \n",
    "        #either shift, left arc or right arc\n",
    "        #\"Shift\" = 2; \"LA\" = 0; \"RA\" = 1\n",
    "        #if head of the second last word is the last word, then leftarc\n",
    "        if (i1 > 0) and (h1 == i0):\n",
    "            return 0  #action is left arc ---> gold_t\n",
    "        #if head of the last word is the second last word, then rightarc\n",
    "        #make sure nothing in the buffer has head with the last word on the stack\n",
    "        #otherwise, we lose the last word.....\n",
    "        elif (i1 >= 0) and (h0 == i1) and \\\n",
    "                (not any([x for x in buf if ex['head'][x] == i0])):\n",
    "            return 1  #right arc\n",
    "        #otherwise shift, if something is left in buffer, otherwise, do nothing....\n",
    "        else:\n",
    "            return None if len(buf) == 0 else 2  #shift\n",
    "        \n",
    "    def parse(self, dataset, eval_batch_size=5000):\n",
    "        sentences = []\n",
    "        sentence_id_to_idx = {}\n",
    "        \n",
    "        for i, example in enumerate(dataset):\n",
    "            \n",
    "            #example['word']=[188, 186, 186, ..., 59]\n",
    "            #n_words=37\n",
    "            #sentence=[1, 2, 3, 4, 5,.., 37]\n",
    "            \n",
    "            n_words = len(example['word']) - 1\n",
    "            sentence = [j + 1 for j in range(n_words)]            \n",
    "            sentences.append(sentence)\n",
    "            \n",
    "            #mapping the object unique id to the i            \n",
    "            #The id is the object's memory address\n",
    "            sentence_id_to_idx[id(sentence)] = i\n",
    "            \n",
    "        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n",
    "        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n",
    "        \n",
    "        UAS = all_tokens = 0.0\n",
    "        with tqdm(total=len(dataset)) as prog:\n",
    "            for i, ex in enumerate(dataset):\n",
    "                head = [-1] * len(ex['word'])\n",
    "                for h, t, in dependencies[i]:\n",
    "                    head[t] = h\n",
    "                for pred_h, gold_h, gold_l, pos in \\\n",
    "                        zip(head[1:], ex['head'][1:], ex['dep'][1:], ex['pos'][1:]):\n",
    "                        assert self.id2tok[pos].startswith(P_PREFIX)\n",
    "                        pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
    "                        if (not self.punct(pos_str)):\n",
    "                            UAS += 1 if pred_h == gold_h else 0\n",
    "                            all_tokens += 1\n",
    "                prog.update(i + 1)\n",
    "        UAS /= all_tokens\n",
    "        return UAS, dependencies\n",
    "    # def num_feature(self) :\n",
    "    #     return self.n_features  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper(object):\n",
    "    def __init__(self, parser, dataset, sentence_id_to_idx):\n",
    "        self.parser = parser\n",
    "        self.dataset = dataset\n",
    "        self.sentence_id_to_idx = sentence_id_to_idx\n",
    "\n",
    "    def predict(self, partial_parses):\n",
    "        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dep,\n",
    "                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n",
    "                for p in partial_parses]\n",
    "        mb_x = np.array(mb_x).astype('int32')\n",
    "        mb_x = torch.from_numpy(mb_x).long()\n",
    "        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n",
    "\n",
    "        pred = self.parser.model(mb_x)\n",
    "        pred = pred.detach().numpy()\n",
    "        \n",
    "        #we need to multiply 10000 with legal labels, to force the model not to make any impossible prediction\n",
    "        #other, when we parse sequentially, sometimes there is nothing in the buffer or stack, thus error....        \n",
    "        pred = np.argmax(pred + 10000 * np.array(mb_l).astype('float32'), 1)\n",
    "        pred = [\"S\" if p == 2 else (\"LA\" if p == 0 else \"RA\") for p in pred]\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a simple function to create ids.....\n",
    "def build_dict(keys, offset=0):\n",
    "    #keys = ['P_PREFIX:IN', 'P_PREFIX:DT', 'P_PREFIX:NNP', 'P_PREFIX:CD', so on...]\n",
    "    #offset is needed because this tok2id has something already inside....\n",
    "    count = Counter()\n",
    "    for key in keys:\n",
    "        count[key] += 1\n",
    "    \n",
    "    #most_common = [('P_PREFIX:NN', 70), ('P_PREFIX:IN', 57), ... , ('P_PREFIX:JJR', 1)]\n",
    "    #we use most_common in case we only want some maximum pos tags....\n",
    "    mc = count.most_common()\n",
    "    \n",
    "    #{'P_PREFIX:NN': 31, 'P_PREFIX:IN': 32, .., 'P_PREFIX:JJR': 62} \n",
    "    return {w[0]: index + offset for (index, w) in enumerate(mc)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the parser `__init__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Building parser\n",
      "True\n",
      "True\n",
      "48\n",
      "took 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"2. Building parser\")\n",
    "start = time.time()\n",
    "parser_full = Parser(train_set, dep_logi=True, pos_logi = True)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the `numericalize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  ['ms.', 'haag', 'plays', 'elianti', '.']\n",
      "Pos:  ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
      "Head:  [2, 3, 0, 3, 3]\n",
      "Dep:  ['compound', 'nsubj', 'root', 'dobj', 'punct']\n"
     ]
    }
   ],
   "source": [
    "#before numericalize\n",
    "print(\"Word: \", train_set[1][\"word\"])\n",
    "print(\"Pos: \",  train_set[1][\"pos\"])\n",
    "print(\"Head: \", train_set[1][\"head\"])\n",
    "print(\"Dep: \",  train_set[1][\"dep\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_n = parser_full.numericalize(train_set)\n",
    "dev_set_n   = parser_full.numericalize(dev_set)\n",
    "test_set_n  = parser_full.numericalize(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  [5156, 304, 1364, 1002, 2144, 87]\n",
      "Pos:  [84, 42, 42, 55, 42, 46]\n",
      "Head:  [-1, 2, 3, 0, 3, 3]\n",
      "Dep:  [-1, 2, 22, 0, 9, 31]\n"
     ]
    }
   ],
   "source": [
    "#before numericalize\n",
    "print(\"Word: \", train_set_n[1][\"word\"])\n",
    "print(\"Pos: \",  train_set_n[1][\"pos\"])\n",
    "print(\"Head: \", train_set_n[1][\"head\"])\n",
    "print(\"Dep: \",  train_set_n[1][\"dep\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Embedding\n",
    "\n",
    "Word embedding length of 50.  In the paper, they applied a custom 50-embedding to all the words, pos, and dependencies.  For pos and dependencies, they claimed that there are some similarities that can be learned as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chaky_embeddings(parsers) :    \n",
    "    print(\"4. Loading pretrained embeddings...\",)\n",
    "    start = time.time()\n",
    "    word_vectors = {}\n",
    "    for line in open(\"data/en-cw.txt\").readlines():\n",
    "        we = line.strip().split() #we = word embeddings - first column: word;  the rest is embedding\n",
    "        word_vectors[we[0]] = [float(x) for x in we[1:]] #{word: [list of 50 numbers], nextword: [another list], so on...}\n",
    "        \n",
    "    #create an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
    "    #we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
    "    embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parsers.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "    for token in parsers.tok2id:\n",
    "            i = parsers.tok2id[token]\n",
    "            if token in word_vectors:\n",
    "                embeddings_matrix[i] = word_vectors[token]\n",
    "            elif token.lower() in word_vectors:\n",
    "                embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "    print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
    "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "    return embeddings_matrix\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Loading pretrained embeddings...\n",
      "Embedding matrix shape (vocab, emb size):  (5157, 50)\n",
      "took 3.05 seconds\n"
     ]
    }
   ],
   "source": [
    "embeddings_matrix_full = Chaky_embeddings(parser_full)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5157"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_matrix_full)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Preprocessing training data...\n",
      "took 1.20 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"5. Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser_full.create_instances(train_set_n)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5155, 5155, 5156, 91, 113]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples[0][0][:5] #features, legal_labels, transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Minibatch loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(data, minibatch_size, shuffle=True):\n",
    "    data_size = len(data[0])\n",
    "    indices = np.arange(data_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
    "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
    "        yield [_minibatch(d, minibatch_indices) for d in data]\n",
    "\n",
    "def _minibatch(data, minibatch_idx):\n",
    "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n",
    "\n",
    "def minibatches(data, batch_size):\n",
    "    x = np.array([d[0] for d in data])\n",
    "    y = np.array([d[2] for d in data])\n",
    "    one_hot = np.zeros((y.size, 3))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return get_minibatches([x, one_hot], batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddings, n_features=48,\n",
    "                 hidden_size=400, n_classes=3, dropout_prob=0.5):\n",
    "\n",
    "        super(ParserModel, self).__init__()\n",
    "        self.n_features   = n_features\n",
    "        self.n_classes    = n_classes\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.embed_size   = embeddings.shape[1]\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
    "        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "\n",
    "        self.embed_to_hidden = nn.Linear(n_features * self.embed_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.hidden_to_logits = nn.Linear(hidden_size, n_classes)\n",
    "\n",
    "    def embedding_lookup(self, t):\n",
    "        #t:  batch_size, n_features\n",
    "        batch_size = t.size()[0]\n",
    "                    \n",
    "        x = self.pretrained_embeddings(t)        \n",
    "        x = x.reshape(-1, self.n_features * self.embed_size)\n",
    "        # x = (1024, 48 * 50)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, t):\n",
    "        # t: (1024, 48)\n",
    "        embeddings = self.embedding_lookup(t)  \n",
    "    \n",
    "        # embeddings: (1024, 48 * 50)\n",
    "        hidden = self.embed_to_hidden(embeddings)\n",
    "    \n",
    "        # hidden: (1024, 200)\n",
    "        hidden_activations = F.relu(hidden)\n",
    "        # hidden_activations: (1024, 200)\n",
    "        thin_net = self.dropout(hidden_activations)\n",
    "        # thin_net: (1024, 200)\n",
    "        logits = self.hidden_to_logits(thin_net)\n",
    "        # logits: (1024, 3)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's code the <code>train_for_epoch</code> and <code>train</code> functions to actually train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just a class to get the average.....\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n",
    "    \n",
    "    best_dev_UAS = 0\n",
    "    \n",
    "    optimizer = optim.Adam(parser.model.parameters(), lr=0.001)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
    "        dev_UAS = train_for_epoch(\n",
    "            parser, train_data, dev_data, optimizer, loss_func, batch_size)\n",
    "        if dev_UAS > best_dev_UAS:\n",
    "            best_dev_UAS = dev_UAS\n",
    "            print(\"New best dev UAS! Saving model.\")\n",
    "            torch.save(parser.model.state_dict(), output_path)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n",
    "    \n",
    "    parser.model.train()  # Places model in \"train\" mode, i.e. apply dropout layer\n",
    "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    with tqdm(total=(n_minibatches)) as prog:\n",
    "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
    "            \n",
    "            #train_x:  batch_size, n_features\n",
    "            #train_y:  batch_size, target(=3)\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "            loss = 0.\n",
    "            train_x = torch.from_numpy(train_x).long()  #long() for int so embedding works....\n",
    "            train_y = torch.from_numpy(train_y.nonzero()[1]).long()  #get the index with 1 because torch expects label to be single integer\n",
    "\n",
    "            # Forward pass: compute predicted logits.\n",
    "            logits = parser.model(train_x)\n",
    "            # Compute loss\n",
    "            loss = loss_func(logits, train_y)\n",
    "            # Compute gradients of the loss w.r.t model parameters.\n",
    "            loss.backward()\n",
    "            # Take step with optimizer.\n",
    "            optimizer.step()\n",
    "\n",
    "            prog.update(1)\n",
    "            loss_meter.update(loss.item())\n",
    "\n",
    "    print(\"Average Train Loss: {}\".format(loss_meter.avg))\n",
    "    print(\"Evaluating on dev set\",)\n",
    "    parser.model.eval()  # Places model in \"eval\" mode, i.e. don't apply dropout layer\n",
    "        \n",
    "    dev_UAS, _ = parser.parse(dev_data)\n",
    "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
    "    return dev_UAS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:06<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.5092427898198366\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7828109.14it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 56.93\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:07<00:00,  6.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.2893778368209799\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7827875.85it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 64.55\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:07<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.22973409884919724\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 8349808.89it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 67.17\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:07<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.19704500461618105\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7828575.75it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 68.66\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:07<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.17327050399035215\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 5963566.95it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 71.48\n",
      "New best dev UAS! Saving model.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create directory if it does not exist for saving the weights...\n",
    "output_dir = \"output/full/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
    "output_path = output_dir + \"model.weights\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "    \n",
    "model_full = ParserModel(embeddings_matrix_full, n_features=48)\n",
    "parser_full.model = model_full\n",
    "\n",
    "start = time.time()\n",
    "train(parser_full, train_examples, dev_set_n, output_path,\n",
    "      batch_size=1024, n_epochs=5, lr=0.0005)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "Final evaluation on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6959114.26it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test UAS: 72.56\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(80 * \"=\")\n",
    "print(\"TESTING\")\n",
    "print(80 * \"=\")\n",
    "\n",
    "print(\"Restoring the best model weights found on the dev set\")\n",
    "parser_full.model.load_state_dict(torch.load(output_path))\n",
    "print(\"Final evaluation on test set\",)\n",
    "parser_full.model.eval()\n",
    "UAS, dependencies = parser_full.parse(test_set_n)\n",
    "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Study\n",
    "* Try to delete only the 12 dep features and check UAS\n",
    "* Try to delete only the 18 pos features and check UAS\n",
    "\n",
    "<br>experiment\n",
    "- what i do is add `dep_logi` and `pos_logi` at Paser object and when you set `dep_logi = False` 12 dep feature will deleted also when you set `pos_logi = False` <br>18 pos feature will deleted."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Try to delete only the 12 dep features and check UAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Building parser\n",
      "False\n",
      "True\n",
      "36\n",
      "took 0.04 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"2. Building parser\")\n",
    "start = time.time()\n",
    "parser_no_dep = Parser(train_set, dep_logi=False, pos_logi = True)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_n = parser_no_dep.numericalize(train_set)\n",
    "dev_set_n   = parser_no_dep.numericalize(dev_set)\n",
    "test_set_n  = parser_no_dep.numericalize(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Loading pretrained embeddings...\n",
      "Embedding matrix shape (vocab, emb size):  (5157, 50)\n",
      "took 3.15 seconds\n"
     ]
    }
   ],
   "source": [
    "embeddings_matrix_no_dep = Chaky_embeddings(parser_no_dep) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Preprocessing training data...\n",
      "took 1.32 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"5. Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser_no_dep.create_instances(train_set_n)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:06<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.4952230869481961\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6592665.82it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 59.63\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:05<00:00,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.266220363167425\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6958376.84it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 63.95\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:05<00:00,  8.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.21479015549023947\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7826942.83it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 67.12\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:05<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.17928949277848005\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7366527.97it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 70.70\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:05<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.1558153689838946\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7368077.76it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 71.83\n",
      "New best dev UAS! Saving model.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create directory if it does not exist for saving the weights...\n",
    "output_dir = \"output/no_dep/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
    "output_path = output_dir + \"model.weights\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "    \n",
    "model2 = ParserModel(embeddings_matrix_no_dep, n_features=36)\n",
    "parser_no_dep.model = model2\n",
    "\n",
    "start = time.time()\n",
    "train(parser_no_dep, train_examples, dev_set_n, output_path,\n",
    "      batch_size=1024, n_epochs=5, lr=0.0005)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing UAS of test set when delete 12 dep feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "Final evaluation on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6958192.51it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test UAS: 74.10\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(80 * \"=\")\n",
    "print(\"TESTING\")\n",
    "print(80 * \"=\")\n",
    "\n",
    "print(\"Restoring the best model weights found on the dev set\")\n",
    "parser_no_dep.model.load_state_dict(torch.load(output_path))\n",
    "print(\"Final evaluation on test set\",)\n",
    "parser_no_dep.model.eval()\n",
    "UAS_no_dep, dependencies = parser_no_dep.parse(test_set_n)\n",
    "print(\"- test UAS: {:.2f}\".format(UAS_no_dep * 100.0))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Try to delete only the 18 pos features and check UAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Building parser\n",
      "True\n",
      "False\n",
      "30\n",
      "took 0.05 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"2. Building parser\")\n",
    "start = time.time()\n",
    "parser_no_pos = Parser(train_set, dep_logi=True, pos_logi = False)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_n = parser_no_pos.numericalize(train_set)\n",
    "dev_set_n   = parser_no_pos.numericalize(dev_set)\n",
    "test_set_n  = parser_no_pos.numericalize(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Loading pretrained embeddings...\n",
      "Embedding matrix shape (vocab, emb size):  (5157, 50)\n",
      "took 3.90 seconds\n"
     ]
    }
   ],
   "source": [
    "embeddings_matrix_no_pos = Chaky_embeddings(parser_no_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Preprocessing training data...\n",
      "took 0.99 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"5. Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser_no_pos.create_instances(train_set_n)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:04<00:00,  9.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.6355643688390652\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7367457.77it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 51.82\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:05<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.3443399810542663\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 5692422.29it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 58.00\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:05<00:00,  8.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.27830755307028693\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 5963499.25it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 61.56\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:05<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.24064521801968417\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6958469.00it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 63.90\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:05<00:00,  9.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.21427006584902605\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6591838.58it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 66.72\n",
      "New best dev UAS! Saving model.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create directory if it does not exist for saving the weights...\n",
    "output_dir = \"output/no_pos/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
    "output_path = output_dir + \"model.weights\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "\n",
    "model = ParserModel(embeddings_matrix_no_pos, n_features=30)\n",
    "parser_no_pos.model = model\n",
    "\n",
    "start = time.time()\n",
    "train(parser_no_pos, train_examples, dev_set_n, output_path,\n",
    "      batch_size=1024, n_epochs=5, lr=0.0005)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "Final evaluation on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6957271.00it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test UAS: 67.62\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(80 * \"=\")\n",
    "print(\"TESTING\")\n",
    "print(80 * \"=\")\n",
    "\n",
    "print(\"Restoring the best model weights found on the dev set\")\n",
    "parser_no_pos.model.load_state_dict(torch.load(output_path))\n",
    "print(\"Final evaluation on test set\",)\n",
    "parser_no_pos.model.eval()\n",
    "UAS_no_pos, dependencies = parser_no_pos.parse(test_set_n)\n",
    "print(\"- test UAS: {:.2f}\".format(UAS_no_pos * 100.0))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|dataset| UAS (dep, pos) |  UAS (pos) | UAS (sep) |\n",
    "|:-- | :--:   |  :--:          | :--:        |\n",
    "|<b>dev_set| 71.48 | 71.83 |  66.72  | \n",
    "|<b>test_set| 72.56 | 74.10 |  67.62  | \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  Chaky embedding vs GloVe \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading data\n"
     ]
    }
   ],
   "source": [
    "train_set, dev_set, test_set = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': ['ms.', 'haag', 'plays', 'elianti', '.'],\n",
       " 'pos': ['NNP', 'NNP', 'VBZ', 'NNP', '.'],\n",
       " 'head': [2, 3, 0, 3, 3],\n",
       " 'dep': ['compound', 'nsubj', 'root', 'dobj', 'punct']}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[1]  # this is what train set look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of word in key dict \"word\"\n",
    "corpus = []\n",
    "for word in train_set :\n",
    "     corpus.append(word[\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ms.', 'haag', 'plays', 'elianti', '.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['links',\n",
       " 'exquisite',\n",
       " 'good',\n",
       " 'accomplished',\n",
       " 'which',\n",
       " 'cheer',\n",
       " 're-elected',\n",
       " 'sees',\n",
       " 'pilgrim',\n",
       " 'last-minute']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist] # use the flatten and vocab code that provide in class\n",
    "vocab = list(set(flatten(corpus))) #got a vocabs of each word\n",
    "vocab[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5069"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numericalization\n",
    "word2index = {w: i for i, w in enumerate(vocab)}\n",
    "voc_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index['<UNK>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "951"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index['policy']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Co occorence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "X_i = Counter(flatten(corpus)) # X_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('an', 'in'), ('an', 'oct.'), ('oct.', 'an'), ('oct.', '19'), ('19', 'oct.')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_grams = []\n",
    "# loop each word sequence\n",
    "# we starts from 1 because 0 has no context\n",
    "# we stop at second last for the same reason\n",
    "for sent in corpus:\n",
    "    for i in range(1, len(sent) - 1):\n",
    "        target = sent[i]\n",
    "        context = [sent[i - 1], sent[i + 1]]\n",
    "        for w in context:\n",
    "            skip_grams.append((target, w))\n",
    "\n",
    "skip_grams[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44720"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ik_skipgram = Counter(skip_grams) # Co-occurece in window size 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Weighting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting(w_i, w_j, X_ik):\n",
    "        \n",
    "    #check whether the co-occurrences exist between these two words\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "    except:\n",
    "        x_ij = 1  #if does not exist, set it to 1, basically smoothing technique\n",
    "                \n",
    "    x_max = 100  # fixed in paper  #cannot exceed 100 counts\n",
    "    alpha = 0.75 # Followed Chaky way!\n",
    "    \n",
    "    #if co-occurrence does not exceed 100, scale it based on some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max)**alpha  #scale it\n",
    "    else:\n",
    "        result = 1  #if is greater than max, set it to 1 maximum\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {}  #for keeping the co-occurences\n",
    "weighting_dic = {} #scaling the percentage of sampling\n",
    "\n",
    "for bigram in combinations_with_replacement(vocab, 2):\n",
    "    if X_ik_skipgram.get(bigram) is not None:  #matches \n",
    "        co_occer = X_ik_skipgram[bigram]  #get the count from what we already counted\n",
    "        X_ik[bigram] = co_occer + 1 # + 1 for stability issue\n",
    "        X_ik[(bigram[1],bigram[0])] = co_occer+1   #count also for the opposite\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)\n",
    "# \n",
    "# print(f\"{X_ik=}\")\n",
    "# print(f\"{weighting_dic=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def random_batch(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    #convert to id since our skip_grams is word, not yet id\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_coocs  = []\n",
    "    random_weightings = []\n",
    "    random_index = np.random.choice(range(len(skip_grams_id)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams_id[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([skip_grams_id[i][1]])  # context word, e.g., 3\n",
    "        \n",
    "        #get cooc\n",
    "        pair = skip_grams[i]\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1\n",
    "        random_coocs.append([math.log(cooc)])\n",
    "        \n",
    "        #get weighting\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append([weighting])\n",
    "                    \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size,embed_size):\n",
    "        super(GloVe,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size) # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size) # out embedding\n",
    "        \n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "    def forward(self, center_words, target_words, coocs, weighting):\n",
    "        center_embeds = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
    "        target_embeds = self.embedding_u(target_words) # [batch_size, 1, emb_size]\n",
    "        \n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)\n",
    "        \n",
    "        inner_product = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        #note that coocs already got log\n",
    "        loss = weighting*torch.pow(inner_product +center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size     = 10 # mini-batch size\n",
    "embedding_size = 50 #so we can later plot\n",
    "model_glove          = GloVe(voc_size, embedding_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 | cost: 140.083832 | time: 0m 0s\n",
      "Epoch: 2000 | cost: 23.872330 | time: 0m 0s\n",
      "Epoch: 3000 | cost: 32.689800 | time: 0m 0s\n",
      "Epoch: 4000 | cost: 38.512352 | time: 0m 0s\n",
      "Epoch: 5000 | cost: 55.116276 | time: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Training\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    input_batch, target_batch, cooc_batch, weighting_batch = random_batch(batch_size, corpus, skip_grams, X_ik, weighting_dic)\n",
    "    input_batch  = torch.LongTensor(input_batch)         #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch)        #[batch_size, 1]\n",
    "    cooc_batch   = torch.FloatTensor(cooc_batch)         #[batch_size, 1]\n",
    "    weighting_batch = torch.FloatTensor(weighting_batch) #[batch_size, 1]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = model_glove(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(word, model):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except :\n",
    "        index = word2index['<UNK>'] \n",
    "    word = torch.LongTensor([index])\n",
    "    \n",
    "    embed =  (model.embedding_v(word)+model.embedding_u(word))/2\n",
    "    return np.array(embed[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.1417243 ,  0.6116885 , -0.5726211 , -0.950994  ,  1.0880558 ,\n",
       "        0.42978895, -1.225419  , -0.5943497 ,  0.03167275, -1.3596511 ,\n",
       "        0.84988153,  0.01764725,  0.17927963,  0.28343362, -0.4388604 ,\n",
       "       -0.58948934, -1.8111966 , -0.4000938 ,  0.4279513 ,  0.90014774,\n",
       "       -0.26865658, -0.2200745 , -0.03649171, -0.92374563, -0.70100963,\n",
       "        0.20497283, -0.49095508, -0.6991694 ,  0.02387497, -1.2568505 ,\n",
       "       -0.40891427, -0.7243005 ,  1.2203032 ,  0.38824025, -0.3812165 ,\n",
       "       -1.4482265 , -0.01481587, -0.27479246, -1.348225  ,  1.0349519 ,\n",
       "       -0.27827185,  0.9703293 ,  1.4579797 , -0.52524126,  0.778848  ,\n",
       "       -1.4880941 ,  0.55404437,  0.08982959, -0.372473  ,  0.68385375],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed(\"car\",model_glove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading data\n"
     ]
    }
   ],
   "source": [
    "_, _, test_set = load_data()\n",
    "# 1. Loading data\n",
    "\n",
    "test_sentence = []\n",
    "for word in test_set :\n",
    "     test_sentence.append(word[\"word\"])\n",
    "test_sentence_1 = \" \".join(test_sentence[16])\n",
    "test_sentence_2 = \" \".join(test_sentence[20]) \n",
    "test_dataset = [test_sentence_1,test_sentence_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at the end of the day , 251.2 million shares were traded .\n",
      "the dow fell 22.6 % on black monday .\n"
     ]
    }
   ],
   "source": [
    "print(test_sentence_1)\n",
    "print(test_sentence_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"56d9edfdc2da40269ee26b86009040f9-0\" class=\"displacy\" width=\"2325\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">end</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">day</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PUNCT</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">251.2</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">million</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">shares</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">were</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">traded</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">PUNCT</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-56d9edfdc2da40269ee26b86009040f9-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,2.0 1975.0,2.0 1975.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-56d9edfdc2da40269ee26b86009040f9-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-56d9edfdc2da40269ee26b86009040f9-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-56d9edfdc2da40269ee26b86009040f9-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-56d9edfdc2da40269ee26b86009040f9-0-2\" stroke-width=\"2px\" d=\"M70,352.0 C70,177.0 390.0,177.0 390.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-56d9edfdc2da40269ee26b86009040f9-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M390.0,354.0 L398.0,342.0 382.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-56d9edfdc2da40269ee26b86009040f9-0-3\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-56d9edfdc2da40269ee26b86009040f9-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M560.0,354.0 L568.0,342.0 552.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-56d9edfdc2da40269ee26b86009040f9-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-56d9edfdc2da40269ee26b86009040f9-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,354.0 L762,342.0 778,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-56d9edfdc2da40269ee26b86009040f9-0-5\" stroke-width=\"2px\" d=\"M595,352.0 C595,177.0 915.0,177.0 915.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-56d9edfdc2da40269ee26b86009040f9-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,354.0 L923.0,342.0 907.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-56d9edfdc2da40269ee26b86009040f9-0-6\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,89.5 1970.0,89.5 1970.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-56d9edfdc2da40269ee26b86009040f9-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,354.0 L1112,342.0 1128,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-56d9edfdc2da40269ee26b86009040f9-0-7\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-56d9edfdc2da40269ee26b86009040f9-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,354.0 L1287,342.0 1303,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-56d9edfdc2da40269ee26b86009040f9-0-8\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,264.5 1610.0,264.5 1610.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-56d9edfdc2da40269ee26b86009040f9-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,354.0 L1462,342.0 1478,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-56d9edfdc2da40269ee26b86009040f9-0-9\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,177.0 1965.0,177.0 1965.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-56d9edfdc2da40269ee26b86009040f9-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,354.0 L1637,342.0 1653,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-56d9edfdc2da40269ee26b86009040f9-0-10\" stroke-width=\"2px\" d=\"M1820,352.0 C1820,264.5 1960.0,264.5 1960.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-56d9edfdc2da40269ee26b86009040f9-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,354.0 L1812,342.0 1828,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-56d9edfdc2da40269ee26b86009040f9-0-11\" stroke-width=\"2px\" d=\"M1995,352.0 C1995,264.5 2135.0,264.5 2135.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-56d9edfdc2da40269ee26b86009040f9-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2135.0,354.0 L2143.0,342.0 2127.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"d854a39059144ffe89ded86290205f07-0\" class=\"displacy\" width=\"1625\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">dow</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">fell</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">22.6</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">%</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">black</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">monday</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">PUNCT</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d854a39059144ffe89ded86290205f07-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d854a39059144ffe89ded86290205f07-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d854a39059144ffe89ded86290205f07-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d854a39059144ffe89ded86290205f07-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d854a39059144ffe89ded86290205f07-0-2\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d854a39059144ffe89ded86290205f07-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d854a39059144ffe89ded86290205f07-0-3\" stroke-width=\"2px\" d=\"M420,352.0 C420,177.0 740.0,177.0 740.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d854a39059144ffe89ded86290205f07-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,354.0 L748.0,342.0 732.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d854a39059144ffe89ded86290205f07-0-4\" stroke-width=\"2px\" d=\"M420,352.0 C420,89.5 920.0,89.5 920.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d854a39059144ffe89ded86290205f07-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M920.0,354.0 L928.0,342.0 912.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d854a39059144ffe89ded86290205f07-0-5\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d854a39059144ffe89ded86290205f07-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,354.0 L1112,342.0 1128,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d854a39059144ffe89ded86290205f07-0-6\" stroke-width=\"2px\" d=\"M945,352.0 C945,177.0 1265.0,177.0 1265.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d854a39059144ffe89ded86290205f07-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,354.0 L1273.0,342.0 1257.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d854a39059144ffe89ded86290205f07-0-7\" stroke-width=\"2px\" d=\"M420,352.0 C420,2.0 1450.0,2.0 1450.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d854a39059144ffe89ded86290205f07-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1450.0,354.0 L1458.0,342.0 1442.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy \n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "for sent in test_dataset:\n",
    "    doc = nlp(sent)\n",
    "    options = {\"collapse_punct\": False}\n",
    "    displacy.render(doc, options = options, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.eval of ParserModel(\n",
       "  (pretrained_embeddings): Embedding(5157, 50)\n",
       "  (embed_to_hidden): Linear(in_features=2400, out_features=400, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (hidden_to_logits): Linear(in_features=400, out_features=3, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser_full.model.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, ?it/s]                   \n"
     ]
    }
   ],
   "source": [
    "UAS_no_pos, dependencies = parser_full.parse(parser_full.numericalize([test_set[16], test_set[20]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(3, 2),\n",
       "  (3, 1),\n",
       "  (6, 5),\n",
       "  (6, 4),\n",
       "  (3, 6),\n",
       "  (9, 8),\n",
       "  (10, 9),\n",
       "  (12, 11),\n",
       "  (12, 10),\n",
       "  (12, 7),\n",
       "  (12, 3),\n",
       "  (12, 13),\n",
       "  (0, 12)],\n",
       " [(2, 1), (3, 2), (5, 4), (3, 5), (8, 7), (8, 6), (3, 8), (3, 9), (0, 3)]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependencies"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image-2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sumarry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation\n",
    "this is the result when we try to delete some feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|dataset| UAS (dep, pos) |  UAS (pos) | UAS (sep) |\n",
    "|:-- | :--:   |  :--:          | :--:        |\n",
    "|<b>dev_set| 71.48 | 71.83 |  66.72  | \n",
    "|<b>test_set| 72.56 | 74.10 |  67.62  | "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UAS score when we try to delete 12 dep features is perform best performance at 71.83 for dev_dataset and 74.10 for test_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Compare"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i random choose 2 sentence and sent it to spacy model and compare with our model, as a result, our model similarity to spacy around 80 percentage"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "3dc66c11351b0102d9b13bb868253b81f2002b43c320e9088049bbc4e6917ed6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
