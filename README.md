# NLP
## Paper Reading
### 1. Graph Transformer Networks (NeurIPS, 2019)
|Problems| Limitation of existing Graph Neural Networks (GNNs) in handling heterogeneous graphs  |
|:------:|:-----|
|Related work| 1. Graph Neural Networks (GNNs) are machine learning models developed to perform various tasks on graphs. There are two approaches, spectral and non-spectral.<br> 2. Node classification with GNNs is used to predict a label for each node in a graph based on its structure and features  |
|Solution| Author porpose a new framework call Graph Transformer Networks(GTNs). The GTNs are designed to transform heterogeneous graphs into meta-path graphs, which are paths connecting different types of edges. The GTNs learn to generate these meta-path graphs and node representations |
|Result|Author compares the performance of different Graph Neural Network (GNN) models for node classification in heterogeneous graphs. The models compared are GCN, GAT, HAN, and the proposed GTN model with public 3 datasets are DBLP, ACM, IMDB. The results show that the GNN-based methods perform better than random walk-based network embedding methods.|
### 2. Comparing word2vec and GloVe for Automatic Measurement of MWE Compositionality (ACL, 2020)
|Problems| The authors aim to compare the effectiveness of two popular embedding methods, GloVe and word2vec, for automatic measurement of the semantic compositionality of multiword expressions task and to explore the suitability of different resources for compositionality assessment. The ultimate goal is to improve the automatic identification and processing of MWEs, which are important but challenging for natural language processing.  |
|:------:|:-----|
|Related work|  |
|Solution|  |
|Result|  |
